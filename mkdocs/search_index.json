{
    "docs": [
        {
            "location": "/",
            "text": "Welcome to ECE 420\n\n\n\n\nCourse Description\n\n\nThe first seven weeks of the course will be structured labs based on fundamental digital signal processing (DSP) concepts from ECE 310. The next two weeks will be on the implementation and simulation of a fundamental DSP algorithm of a student's choosing from a set of seminal DSP papers (such as adaptive filtering, pitch detection, edge-aware filtering, motion tracking, pattern recognition, etc). The remaining six weeks in the course will revolve around the development, testing, and documentation of a DSP project of the student's choice (subject to instructor approval).\n\n\nCourse Goals\n\n\nStudents will learn to prototype, implement, and analyze real-time DSP systems. Students will both broaden and deepen their understanding of basic DSP theory and techniques and learn to relate this understanding to real-world observations and applications. Students will learn industrially-relevant skills such as rapid design prototyping in Python, and Android development of DSP applications for computationally-constrained mobile devices. Other significant educational experiences include open-ended design, oral, and written communication, and team projects.\n\n\nCourse Schedule\n\n\nLectures are once a week on Monday from 2:00-2:50pm in 4070 ECEB.\n\n\n\n\n\n\n\n\nWeek of\n\n\nLecture Topic\n\n\nLab\n\n\nDue (in lab)\n\n\n\n\n\n\n\n\n\n\n01/16\n\n\nNo lecture - MLK Day\n\n\nLab 1 - IMU Pedometer\n\n\nNone\n\n\n\n\n\n\n01/23\n\n\nOverview of DSP Systems\n\n\nLab 2 - Real-time Audio Filtering\n\n\nPrelab 2, Demo and Quiz 1\n\n\n\n\n\n\n01/30\n\n\nShort-time Spectral Analysis\n\n\nLab 3 - Spectrogram\n\n\nPrelab 3, Demo and Quiz 2\n\n\n\n\n\n\n02/06\n\n\nCorrelation Analysis\n\n\nLab 4 - Pitch Detection\n\n\nPrelab 4, Demo and Quiz 3\n\n\n\n\n\n\n02/13\n\n\nPitch Modification\n\n\nLab 5 - Autotune\n\n\nPrelab 5, Demo and Quiz 4\n\n\n\n\n\n\n02/20\n\n\nImage Processing\n\n\nLab 6 - Image Processing\n\n\nPrelab 6, Demo and Quiz 5\n\n\n\n\n\n\n02/27\n\n\nVideo Processing\n\n\nLab 7 - Video Processing\n\n\nPrelab 7, Demo and Quiz 6\n\n\n\n\n\n\n03/06\n\n\nAdaptive Filter.  LMS Algorithm\n\n\nAssigned Project Lab\n\n\nDemo and Quiz 7, Final Project Proposal\n\n\n\n\n\n\n03/13\n\n\nKalman Filter\n\n\nAssigned Project Lab\n\n\nAssigned Project Demo\n\n\n\n\n\n\n03/20\n\n\nSpring Break\n\n\n\n\n\n\n\n\n\n\n03/27\n\n\nChange Detection\n\n\nDesign Review\n\n\nRevised Project Proposal due by end of week\n\n\n\n\n\n\n04/03\n\n\nObject Detection\n\n\nFinal Project\n\n\nMilestone 1\n\n\n\n\n\n\n04/10\n\n\nObject Tracking\n\n\nFinal Project\n\n\nMilestone 2\n\n\n\n\n\n\n04/17\n\n\nObject Recognition\n\n\nFinal Project\n\n\nMilestone 3\n\n\n\n\n\n\n04/24\n\n\nEmbedded Systems\n\n\nFinal Project Presentations\n\n\nFinal Demo\n\n\n\n\n\n\n05/01\n\n\nReview.  In-class Quiz\n\n\nNo labs\n\n\nFinal Project Report due by Reading Day\n\n\n\n\n\n\n\n\nLabs\n\n\nLabs are held in Room 5072 ECEB.\n\n\n\n\nSection ABA meets Tuesday, 2:00-3:50 PM.\n\n\nSection ABC meets Wednesday, 2:00-3:50 PM.\n\n\nSection ABD meets Thursday, 2:00-3:50 PM.\n\n\nSection ABE meets Friday, 2:00-3:50 PM.\n\n\n\n\nWorking code is due the same day as the quiz.\n\n\nThe lab is available at all times except University holiday weekends and ECE 420 class times.\n\n\nStudents are expected to be in the lab for their two-hour assigned lab period. In addition, students can access the lab at any time (subject to departmental rules for normal lab privileges) using their I-card. It is expected that students will require additional lab time to complete their assignments. ECE 420 students having difficulty with their I-card access should notify one of the teaching assistants.\n\n\nBasic rules of courtesy and professional behavior are expected in the lab. Please do not remove any lab equipment, books, or manuals from the lab at any time. If you would like to listen to music as you work, please use headphones.\n\n\nGrades\n\n\nGrades can be found on \nCompass 2g\n.\n\n\nBrief Grading Breakdown\n\n\n\n\nStructured Labs - 40%\n\n\n4 points for the prelab and quiz\n\n\n4 points for the demo\n\n\n\n\n\n\nAssigned Project Lab - 10%\n\n\n5% for the report\n\n\n5% for the demo\n\n\n\n\n\n\nFinal Project - 45%\n\n\n10% for the project proposal and design review\n\n\n15% for the completion of three milestones\n\n\n10% for the final demo\n\n\n10% for the final report\n\n\n\n\n\n\nFinal Lecture Quiz - 5%\n\n\n\n\nDetailed Grading Breakdown\n\n\nThe structured laboratory segment will count for 40% of the total grade, based on completion of, and oral examination over, the weekly laboratory assignments, including the underlying theory, details of the implementation and code, and the observed behavior of the system.\n\n\nEach lab is worth 8 points, usually with 4 point for prelab and written quiz, and the remaining 4 points for demo and oral quiz. We emphasize that your grade is based heavily on your understanding and demonstration of the course material, not just on submitting working code.\n\n\nThe assigned project lab (based on the student's chosen DSP paper) will account for 10% of the total grade.\n\n\nThe final project will count for 45% of the total grade, with 10% on the project proposal and the design review, 15% for demonstrations and quizzes of 3 project milestones, 10% for the final demo and oral presentations, and 10% on the final report.\n\n\nThe final 5% of the total course grade comes from a written quiz over the lecture material.\n\n\nIt is expected that each student will attend and participate in scheduled class and laboratory meetings, or will make prior alternate arrangements with the instructor. The final grade may be penalized if this does not occur.\n\n\nAll assignments other than the lecture quiz, final project proposal, and final project report are due during the scheduled laboratory meeting. A late penalty of 50% will be assessed for assignments less than a week late; assignments more than a week late will receive no credit. However, all graded assignments must be submitted to receive a passing grade in the course.\n\n\nProject Details\n\n\nProject details TBA.\n\n\n\n\n\nAcademic Integrity Policy\n\n\nPrinted and online sources are allowed with proper citation. Please direct your question to Google or the course staff before you ask your classmates. Given the range of the material for this course, we encourage you to refer to any online source, but do not directly copy and paste.\n\n\nWe do not allow inter-group cooperation for the final project. If there is a sign of cooperation between groups, those groups will be treated as a big group, and the grade will be divided accordingly.\n\n\nMore information: \nStudent Code\n.\n\n\nOffice Hours / Course Administrivia\n\n\n\n\nProf. Minh Do: Monday, 3-4pm, 113 CSL\n\n\nTA Dario Aranguiz: Monday, 4-5pm, 5072 ECEB\n\n\nTA Trong Nguyen: Wednesday, 1-2pm, 5072 ECEB\n\n\nTA Dongbo Wang: Friday, 12-2pm, 5072 ECEB\n\n\n\n\nIf you have questions, post them on \nPiazza\n.\n\n\nInstructor Contact Information\n\n\n\n\nProf. Minh Do:  minhdo AT illinois DOT edu\n\n\nTA Dario Aranguiz: arangui2\n\n\nTA Trong Nguyen: tnnguyn2\n\n\nTA Dongbo Wang: dwang49",
            "title": "Home"
        },
        {
            "location": "/#welcome-to-ece-420",
            "text": "",
            "title": "Welcome to ECE 420"
        },
        {
            "location": "/#course-description",
            "text": "The first seven weeks of the course will be structured labs based on fundamental digital signal processing (DSP) concepts from ECE 310. The next two weeks will be on the implementation and simulation of a fundamental DSP algorithm of a student's choosing from a set of seminal DSP papers (such as adaptive filtering, pitch detection, edge-aware filtering, motion tracking, pattern recognition, etc). The remaining six weeks in the course will revolve around the development, testing, and documentation of a DSP project of the student's choice (subject to instructor approval).",
            "title": "Course Description"
        },
        {
            "location": "/#course-goals",
            "text": "Students will learn to prototype, implement, and analyze real-time DSP systems. Students will both broaden and deepen their understanding of basic DSP theory and techniques and learn to relate this understanding to real-world observations and applications. Students will learn industrially-relevant skills such as rapid design prototyping in Python, and Android development of DSP applications for computationally-constrained mobile devices. Other significant educational experiences include open-ended design, oral, and written communication, and team projects.",
            "title": "Course Goals"
        },
        {
            "location": "/#course-schedule",
            "text": "Lectures are once a week on Monday from 2:00-2:50pm in 4070 ECEB.     Week of  Lecture Topic  Lab  Due (in lab)      01/16  No lecture - MLK Day  Lab 1 - IMU Pedometer  None    01/23  Overview of DSP Systems  Lab 2 - Real-time Audio Filtering  Prelab 2, Demo and Quiz 1    01/30  Short-time Spectral Analysis  Lab 3 - Spectrogram  Prelab 3, Demo and Quiz 2    02/06  Correlation Analysis  Lab 4 - Pitch Detection  Prelab 4, Demo and Quiz 3    02/13  Pitch Modification  Lab 5 - Autotune  Prelab 5, Demo and Quiz 4    02/20  Image Processing  Lab 6 - Image Processing  Prelab 6, Demo and Quiz 5    02/27  Video Processing  Lab 7 - Video Processing  Prelab 7, Demo and Quiz 6    03/06  Adaptive Filter.  LMS Algorithm  Assigned Project Lab  Demo and Quiz 7, Final Project Proposal    03/13  Kalman Filter  Assigned Project Lab  Assigned Project Demo    03/20  Spring Break      03/27  Change Detection  Design Review  Revised Project Proposal due by end of week    04/03  Object Detection  Final Project  Milestone 1    04/10  Object Tracking  Final Project  Milestone 2    04/17  Object Recognition  Final Project  Milestone 3    04/24  Embedded Systems  Final Project Presentations  Final Demo    05/01  Review.  In-class Quiz  No labs  Final Project Report due by Reading Day",
            "title": "Course Schedule"
        },
        {
            "location": "/#labs",
            "text": "Labs are held in Room 5072 ECEB.   Section ABA meets Tuesday, 2:00-3:50 PM.  Section ABC meets Wednesday, 2:00-3:50 PM.  Section ABD meets Thursday, 2:00-3:50 PM.  Section ABE meets Friday, 2:00-3:50 PM.   Working code is due the same day as the quiz.  The lab is available at all times except University holiday weekends and ECE 420 class times.  Students are expected to be in the lab for their two-hour assigned lab period. In addition, students can access the lab at any time (subject to departmental rules for normal lab privileges) using their I-card. It is expected that students will require additional lab time to complete their assignments. ECE 420 students having difficulty with their I-card access should notify one of the teaching assistants.  Basic rules of courtesy and professional behavior are expected in the lab. Please do not remove any lab equipment, books, or manuals from the lab at any time. If you would like to listen to music as you work, please use headphones.",
            "title": "Labs"
        },
        {
            "location": "/#grades",
            "text": "Grades can be found on  Compass 2g .",
            "title": "Grades"
        },
        {
            "location": "/#brief-grading-breakdown",
            "text": "Structured Labs - 40%  4 points for the prelab and quiz  4 points for the demo    Assigned Project Lab - 10%  5% for the report  5% for the demo    Final Project - 45%  10% for the project proposal and design review  15% for the completion of three milestones  10% for the final demo  10% for the final report    Final Lecture Quiz - 5%",
            "title": "Brief Grading Breakdown"
        },
        {
            "location": "/#detailed-grading-breakdown",
            "text": "The structured laboratory segment will count for 40% of the total grade, based on completion of, and oral examination over, the weekly laboratory assignments, including the underlying theory, details of the implementation and code, and the observed behavior of the system.  Each lab is worth 8 points, usually with 4 point for prelab and written quiz, and the remaining 4 points for demo and oral quiz. We emphasize that your grade is based heavily on your understanding and demonstration of the course material, not just on submitting working code.  The assigned project lab (based on the student's chosen DSP paper) will account for 10% of the total grade.  The final project will count for 45% of the total grade, with 10% on the project proposal and the design review, 15% for demonstrations and quizzes of 3 project milestones, 10% for the final demo and oral presentations, and 10% on the final report.  The final 5% of the total course grade comes from a written quiz over the lecture material.  It is expected that each student will attend and participate in scheduled class and laboratory meetings, or will make prior alternate arrangements with the instructor. The final grade may be penalized if this does not occur.  All assignments other than the lecture quiz, final project proposal, and final project report are due during the scheduled laboratory meeting. A late penalty of 50% will be assessed for assignments less than a week late; assignments more than a week late will receive no credit. However, all graded assignments must be submitted to receive a passing grade in the course.",
            "title": "Detailed Grading Breakdown"
        },
        {
            "location": "/#project-details",
            "text": "Project details TBA.",
            "title": "Project Details"
        },
        {
            "location": "/#academic-integrity-policy",
            "text": "Printed and online sources are allowed with proper citation. Please direct your question to Google or the course staff before you ask your classmates. Given the range of the material for this course, we encourage you to refer to any online source, but do not directly copy and paste.  We do not allow inter-group cooperation for the final project. If there is a sign of cooperation between groups, those groups will be treated as a big group, and the grade will be divided accordingly.  More information:  Student Code .",
            "title": "Academic Integrity Policy"
        },
        {
            "location": "/#office-hours-course-administrivia",
            "text": "Prof. Minh Do: Monday, 3-4pm, 113 CSL  TA Dario Aranguiz: Monday, 4-5pm, 5072 ECEB  TA Trong Nguyen: Wednesday, 1-2pm, 5072 ECEB  TA Dongbo Wang: Friday, 12-2pm, 5072 ECEB   If you have questions, post them on  Piazza .",
            "title": "Office Hours / Course Administrivia"
        },
        {
            "location": "/#instructor-contact-information",
            "text": "Prof. Minh Do:  minhdo AT illinois DOT edu  TA Dario Aranguiz: arangui2  TA Trong Nguyen: tnnguyn2  TA Dongbo Wang: dwang49",
            "title": "Instructor Contact Information"
        },
        {
            "location": "/lab2/prelab/",
            "text": "Prelab 2 - Notch filter design in Python\n\n\nDownload the \nwith_hum.wav\n and \nwithout_hum.wav\n file\nto your computer.\nFrom the command line, type:\n\n\njupyter notebook\n\n\n\n\n\nto open the \njupyter notebook\n in your browser:\n\n\n\n  \n\n\n\n\n\nThe notebook is used for quickly prototyping your ideas. It also supports inline\nplotting and Markdown/Latex documentation.\n\n\nInside the just-opened webpage, Choose New -> Python[default]. A jupyter notebook\nwill be opened in a new tab.\n\n\nGo to File->Rename and rename it to Prelab 2.\n\n\nIn the first cell in the opened notebook, type \nls\n and press Shift + Enter. It should list all the file in your current directory.\n\n\nMake sure that you are in the same folder where the wav files are stored.\n\n\nTo import the wave file, you might use:\n\n\nfrom\n \nscipy.io.wavfile\n \nimport\n \nread\n\n\nsampling_rate\n,\n \ndata\n \n=\n \nread\n(\n'with_hum.wav'\n)\n\n\n\n\n\n\nAnd to play the audio file:\n\n\nfrom\n \nIPython.display\n \nimport\n \nAudio\n\n\nAudio\n(\n'with_hum.wav'\n)\n\n\n\n\n\n\nPlay both wav files to see the difference. \n\n\nExercise 1\n\n\nPlot the magnitude of the frequency response of the signal with the 400 Hz humming (with_hum.wav). You need to include this line at the beginning of the notebook to make the plot visible in the notebook:\n\n\n%matplotlib\n \ninline\n\n\n\n\n\n\n\n\nHint\n\n\nuse numpy fft function.\n\n\n\n\nExcercise 2\n\n\nDesign a bandstop filter to filter out the humming interference. Plot the magniture and phase of the frequency response of the filter.\n\n\n\n\nHint\n\n\ntake a look at butter and freqz functions from the scipy.signal module.\n\n\n\n\nExcercise 3\n\n\nFilter the 400 Hz humming interference out. Plot the frequency response of the filtered signal.\nPlay the filtered signal again. You should hear the humming interference died out.\n\n\n\n\nHint\n\n\nuse lfilter function from scipy.signal module.\n\n\n\n\n\n\nTip\n\n\nyou can choose File -> Download as to save the notebook as a python file to debug in PyCharm.\n\n\n\n\nSubmission\n\n\nEmail your Jupyter notebook to your TA before the beginning of your lab section. Ensure that your notebook contains the following:\n\n\n\n\nA plot of the magnitude of the frequency response of the original signal (with the 400 Hz humming)\n\n\nA plot of the magnitude and phase of your bandstop filter's frequency response\n\n\nA plot of the magnitude of the frequency response of the filtered signal (with the 400 Hz humming canceled out)\n\n\nA brief discussion about your filter design. Why does your filter have the number of taps that it has? Could you achieve the same effect with less taps? What are the practical effects of using less taps?",
            "title": "Prelab 2 - IIR Filtering"
        },
        {
            "location": "/lab2/prelab/#prelab-2-notch-filter-design-in-python",
            "text": "Download the  with_hum.wav  and  without_hum.wav  file\nto your computer.\nFrom the command line, type:  jupyter notebook  to open the  jupyter notebook  in your browser:  \n     The notebook is used for quickly prototyping your ideas. It also supports inline\nplotting and Markdown/Latex documentation.  Inside the just-opened webpage, Choose New -> Python[default]. A jupyter notebook\nwill be opened in a new tab.  Go to File->Rename and rename it to Prelab 2.  In the first cell in the opened notebook, type  ls  and press Shift + Enter. It should list all the file in your current directory.  Make sure that you are in the same folder where the wav files are stored.  To import the wave file, you might use:  from   scipy.io.wavfile   import   read  sampling_rate ,   data   =   read ( 'with_hum.wav' )   And to play the audio file:  from   IPython.display   import   Audio  Audio ( 'with_hum.wav' )   Play both wav files to see the difference.",
            "title": "Prelab 2 - Notch filter design in Python"
        },
        {
            "location": "/lab2/prelab/#exercise-1",
            "text": "Plot the magnitude of the frequency response of the signal with the 400 Hz humming (with_hum.wav). You need to include this line at the beginning of the notebook to make the plot visible in the notebook:  %matplotlib   inline    Hint  use numpy fft function.",
            "title": "Exercise 1"
        },
        {
            "location": "/lab2/prelab/#excercise-2",
            "text": "Design a bandstop filter to filter out the humming interference. Plot the magniture and phase of the frequency response of the filter.   Hint  take a look at butter and freqz functions from the scipy.signal module.",
            "title": "Excercise 2"
        },
        {
            "location": "/lab2/prelab/#excercise-3",
            "text": "Filter the 400 Hz humming interference out. Plot the frequency response of the filtered signal.\nPlay the filtered signal again. You should hear the humming interference died out.   Hint  use lfilter function from scipy.signal module.    Tip  you can choose File -> Download as to save the notebook as a python file to debug in PyCharm.",
            "title": "Excercise 3"
        },
        {
            "location": "/lab2/prelab/#submission",
            "text": "Email your Jupyter notebook to your TA before the beginning of your lab section. Ensure that your notebook contains the following:   A plot of the magnitude of the frequency response of the original signal (with the 400 Hz humming)  A plot of the magnitude and phase of your bandstop filter's frequency response  A plot of the magnitude of the frequency response of the filtered signal (with the 400 Hz humming canceled out)  A brief discussion about your filter design. Why does your filter have the number of taps that it has? Could you achieve the same effect with less taps? What are the practical effects of using less taps?",
            "title": "Submission"
        },
        {
            "location": "/lab3/prelab/",
            "text": "Prelab 3\n\n\nSummary\n\n\nYou will investigate the effects of windowing and zero-padding on the Discrete Fourier Transform of a signal, as well as the effects of data-set quantities and weighting windows used in Power Spectral Density Estimation.\n\n\nSubmission Instructions\n\n\nPlease create a Jupyter notebook (\n.ipynb\n file) and address the assignment blocks below. Email the file to your TA before the start of your lab section.\n\n\nExercise 1\n\n\nSince the DFT is a sampled version of the spectrum of a digital signal, it has certain sampling effects. To explore these sampling effects more thoroughly, we consider the effect of multiplying the time signal by different window functions and the effect of using zero-padding to increase the length (and thus the number of sample points) of the DFT. Using the following Python script as an example, plot the squared-magnitude response of the following test cases over the digital frequencies \n\\omega_c=[\\frac{\\pi}{8},\\frac{3\\pi}{8}]\n.\n\n\n\n\nRectangular window with no zero-padding\n\n\nHamming window with no zero-padding\n\n\nRectangular window with zero-padding by factor of four (i.e., 1024-point FFT)\n\n\nHamming window window with zero-padding by factor of four\n\n\n\n\nimport\n \nnumpy\n \nas\n \nnp\n\n\nimport\n \nmatplotlib.pyplot\n \nas\n \nplt\n\n\nfrom\n \nscipy\n \nimport\n \nsignal\n\n\n#%matplotlib inline      # Uncomment this to show figure in Jupyter Notebook\n\n\n\nN\n \n=\n \n256\n;\n                 \n# length of test signals\n\n\nnum_freqs\n \n=\n \n100\n;\n         \n# number of frequencies to test\n\n\n\n# Generate vector of frequencies to test\n\n\nomega\n \n=\n \nnp\n.\npi\n/\n8\n \n+\n \nnp\n.\nlinspace\n(\n0\n,\nnum_freqs\n-\n1\n,\nnum_freqs\n)\n/\nnum_freqs\n*\nnp\n.\npi\n/\n4\n;\n\n\n\nS\n \n=\n \nnp\n.\nzeros\n([\nN\n,\nnum_freqs\n]);\n                        \n# matrix to hold FFT results\n\n\n\nfor\n \ni\n \nin\n \nrange\n(\n0\n,\nlen\n(\nomega\n)):\n                       \n# loop through freq. vector\n\n    \ns\n \n=\n \nnp\n.\nsin\n(\nomega\n[\ni\n]\n*\nnp\n.\nlinspace\n(\n0\n,\nN\n-\n1\n,\nN\n));\n      \n# generate test sine wave\n\n    \nwin\n \n=\n \nsignal\n.\nboxcar\n(\nN\n);\n                         \n# use rectangular window\n\n    \ns\n \n=\n \ns\n*\nwin\n;\n                                      \n# multiply input by window\n\n    \nS\n[:,\ni\n]\n \n=\n \nnp\n.\nsquare\n(\nnp\n.\nabs\n(\nnp\n.\nfft\n.\nfft\n(\ns\n)));\n      \n# generate magnitude of FFT\n\n                                                    \n# and store as a column of S\n\n\n\nplt\n.\nplot\n(\nS\n);\n                                        \n# plot all spectra on same graph\n\n\n\n\n\n\n\n\nAssignment 1\n\n\nDescribe the tradeoff between mainlobe width and sidelobe behavior for the various window functions. Does zero-padding increase frequency resolution? Are we getting something for free? What is the relationship between the DFT, \nX[k]\n, and the DTFT, \nX(\u03c9)\n, of a sequence \nx[n] \n?\n\n\n\n\nExercise 2.\n\n\nIn this section, you will resolve the two closely spaced sine waves using a Fourier transform method. Consider the signal:\n\n\n\n\n x(n)=sin(2\\pi f_1 n)+sin(2\\pi f_2 n) \n\n\n\n\nconsisting of two sine waves of frequency 2000 Hz and 2100 Hz with sampling frequency of 8000 Hz. Here, n is the discrete time index.\n\n\n\n\nAssignment 2\n\n\nGenerate a block of 256 samples of x(n) and use the Fast Fourier Transform (fft) command to determine the two frequency components. Plot the magnitude of the frequency output.\n\n\n\n\n\n\nAssignment 3\n\n\nWhat is the closest frequency to 2000 Hz that you can resolve using the Fourier transform method? Which of the following method applied to x(n) results in the best resolving capabilities? Why?\n\n\n\n\nRectangular window with no zero-padding\n\n\nHamming window with no zero-padding\n\n\nRectangular window with zero-padding by factor of four (i.e., 1024-point FFT)\n\n\nHamming window window with zero-padding by factor of four\n\n\n\n\n\n\nExercise 3:\n\n\nThe folling Python code can be used to generate a pure tone:\n\n\nimport\n \nnumpy\n \nas\n \nnp\n\n\nfrom\n \nIPython.display\n \nimport\n \nAudio\n\n\n\nfs\n \n=\n \n8000\n        \n# Sampling Rate is 8000\n\n\nduration\n \n=\n \n1\n     \n# 1 sec\n\n\nt\n \n=\n \nnp\n.\nlinspace\n(\n0\n,\nduration\n,\nduration\n*\nfs\n)\n\n\nfreq\n \n=\n \n600\n       \n# Tune Frequency is 600Hz\n\n\ntune\n \n=\n \nnp\n.\nsin\n(\n2\n*\nnp\n.\npi\n*\nfreq\n*\nt\n)\n\n\n\n# To listen to it, you can use:\n\n\nAudio\n(\ntune\n,\nrate\n=\nfs\n)\n\n\n\n\n\n\n Short-time spectral analysis \n is an important technique that is used to visualize the time evolution of the frequency content of non-stationary signals, such as speech. The fundamental assumption is that the signal is modeled as being quasi-stationary over short time periods; in many speech applications, this period is on the order of \n 20-30 milliseconds \n.\n\n\nThe short-time Fourier transform (STFT) is defined to be:\n\n X(\\tau,f)=FT[x(t)w(t\u2212\\tau)] \n\n\n\n\nwhere f is frequency, FT represents the Fourier transform , and \n w(t\u2212\\tau) \n\n is a window with finite-time support centered at time \n\\tau\n\n (i.e., it is non-zero for only a short amount of time near\n\\tau\n.\n\n\nIn the case of digital time and frequency, the rate at which \n\\tau\n is evaluated, along with the block size used to compute the FFT determines the amount of data overlap that occurs in evaluating the STFT over time.\n\n\nThe spectrogram is just the magnitude-squared of the STFT, and can be computed in Python using scipy.signal.spectrogram.\n\n\nThe question of how much to overlap can be understood by asking how many time/frequency samples are required to fully represent the continuous time-frequency STFT; for the interested reader, see the section titled \u201cAnalysis of Short Term Spectra\u201d in \nAllen77\n.\n\n\n\n\nAssignment 4\n\n\nPlot the spectrogram of the first two signals you generate in part 2 with no overlap and 50% overlap. How are the spectrograms different between the two methods?\n\n\nRepeat this for the the following frequency-sweep signal:\n\n\nimport\n \nnumpy\n \nas\n \nnp\n\n\nfrom\n \nIPython.display\n \nimport\n \nAudio\n\n\nfrom\n \nscipy\n \nimport\n \nsignal\n\n\n\nt\n \n=\n \nnp\n.\nlinspace\n(\n0\n,\n0.5\n,\n4001\n)\n\n\ns\n \n=\n \nsignal\n.\nchirp\n(\nt\n,\n1000\n,\n0.5\n,\n5000\n);\n    \n# Frequency-sweep signal\n\n\n\nAudio\n(\ns\n,\nrate\n=\n8192\n)\n    \n# Default rate is 8192Hz\n\n\n\n\n\n\nWhat is going on at 0.4 seconds into the signal?",
            "title": "Prelab 3 - Fourier Transforms"
        },
        {
            "location": "/lab3/prelab/#prelab-3",
            "text": "",
            "title": "Prelab 3"
        },
        {
            "location": "/lab3/prelab/#summary",
            "text": "You will investigate the effects of windowing and zero-padding on the Discrete Fourier Transform of a signal, as well as the effects of data-set quantities and weighting windows used in Power Spectral Density Estimation.",
            "title": "Summary"
        },
        {
            "location": "/lab3/prelab/#submission-instructions",
            "text": "Please create a Jupyter notebook ( .ipynb  file) and address the assignment blocks below. Email the file to your TA before the start of your lab section.",
            "title": "Submission Instructions"
        },
        {
            "location": "/lab3/prelab/#exercise-1",
            "text": "Since the DFT is a sampled version of the spectrum of a digital signal, it has certain sampling effects. To explore these sampling effects more thoroughly, we consider the effect of multiplying the time signal by different window functions and the effect of using zero-padding to increase the length (and thus the number of sample points) of the DFT. Using the following Python script as an example, plot the squared-magnitude response of the following test cases over the digital frequencies  \\omega_c=[\\frac{\\pi}{8},\\frac{3\\pi}{8}] .   Rectangular window with no zero-padding  Hamming window with no zero-padding  Rectangular window with zero-padding by factor of four (i.e., 1024-point FFT)  Hamming window window with zero-padding by factor of four   import   numpy   as   np  import   matplotlib.pyplot   as   plt  from   scipy   import   signal  #%matplotlib inline      # Uncomment this to show figure in Jupyter Notebook  N   =   256 ;                   # length of test signals  num_freqs   =   100 ;           # number of frequencies to test  # Generate vector of frequencies to test  omega   =   np . pi / 8   +   np . linspace ( 0 , num_freqs - 1 , num_freqs ) / num_freqs * np . pi / 4 ;  S   =   np . zeros ([ N , num_freqs ]);                          # matrix to hold FFT results  for   i   in   range ( 0 , len ( omega )):                         # loop through freq. vector \n     s   =   np . sin ( omega [ i ] * np . linspace ( 0 , N - 1 , N ));        # generate test sine wave \n     win   =   signal . boxcar ( N );                           # use rectangular window \n     s   =   s * win ;                                        # multiply input by window \n     S [:, i ]   =   np . square ( np . abs ( np . fft . fft ( s )));        # generate magnitude of FFT \n                                                     # and store as a column of S  plt . plot ( S );                                          # plot all spectra on same graph    Assignment 1  Describe the tradeoff between mainlobe width and sidelobe behavior for the various window functions. Does zero-padding increase frequency resolution? Are we getting something for free? What is the relationship between the DFT,  X[k] , and the DTFT,  X(\u03c9) , of a sequence  x[n]  ?",
            "title": "Exercise 1"
        },
        {
            "location": "/lab3/prelab/#exercise-2",
            "text": "In this section, you will resolve the two closely spaced sine waves using a Fourier transform method. Consider the signal:    x(n)=sin(2\\pi f_1 n)+sin(2\\pi f_2 n)    consisting of two sine waves of frequency 2000 Hz and 2100 Hz with sampling frequency of 8000 Hz. Here, n is the discrete time index.   Assignment 2  Generate a block of 256 samples of x(n) and use the Fast Fourier Transform (fft) command to determine the two frequency components. Plot the magnitude of the frequency output.    Assignment 3  What is the closest frequency to 2000 Hz that you can resolve using the Fourier transform method? Which of the following method applied to x(n) results in the best resolving capabilities? Why?   Rectangular window with no zero-padding  Hamming window with no zero-padding  Rectangular window with zero-padding by factor of four (i.e., 1024-point FFT)  Hamming window window with zero-padding by factor of four",
            "title": "Exercise 2."
        },
        {
            "location": "/lab3/prelab/#exercise-3",
            "text": "The folling Python code can be used to generate a pure tone:  import   numpy   as   np  from   IPython.display   import   Audio  fs   =   8000          # Sampling Rate is 8000  duration   =   1       # 1 sec  t   =   np . linspace ( 0 , duration , duration * fs )  freq   =   600         # Tune Frequency is 600Hz  tune   =   np . sin ( 2 * np . pi * freq * t )  # To listen to it, you can use:  Audio ( tune , rate = fs )    Short-time spectral analysis   is an important technique that is used to visualize the time evolution of the frequency content of non-stationary signals, such as speech. The fundamental assumption is that the signal is modeled as being quasi-stationary over short time periods; in many speech applications, this period is on the order of   20-30 milliseconds  .  The short-time Fourier transform (STFT) is defined to be:  X(\\tau,f)=FT[x(t)w(t\u2212\\tau)]    where f is frequency, FT represents the Fourier transform , and   w(t\u2212\\tau)  \n is a window with finite-time support centered at time  \\tau \n (i.e., it is non-zero for only a short amount of time near \\tau .  In the case of digital time and frequency, the rate at which  \\tau  is evaluated, along with the block size used to compute the FFT determines the amount of data overlap that occurs in evaluating the STFT over time.  The spectrogram is just the magnitude-squared of the STFT, and can be computed in Python using scipy.signal.spectrogram.  The question of how much to overlap can be understood by asking how many time/frequency samples are required to fully represent the continuous time-frequency STFT; for the interested reader, see the section titled \u201cAnalysis of Short Term Spectra\u201d in  Allen77 .   Assignment 4  Plot the spectrogram of the first two signals you generate in part 2 with no overlap and 50% overlap. How are the spectrograms different between the two methods?  Repeat this for the the following frequency-sweep signal:  import   numpy   as   np  from   IPython.display   import   Audio  from   scipy   import   signal  t   =   np . linspace ( 0 , 0.5 , 4001 )  s   =   signal . chirp ( t , 1000 , 0.5 , 5000 );      # Frequency-sweep signal  Audio ( s , rate = 8192 )      # Default rate is 8192Hz   What is going on at 0.4 seconds into the signal?",
            "title": "Exercise 3:"
        },
        {
            "location": "/lab4/prelab/",
            "text": "Prelab 4\n\n\nSummary\n\n\nIn this prelab, you get familiarized with two common tasks in speech signal analysis: voicing determination and autocorrelation.\n\n\nSubmit the assignments as a jupyter notebook to your respective TAs.\n\n\nDownloads\n\n\n.wav test vector\n\n\nVoiced/Unvoiced Detector\n\n\nVoiced/unvoiced signal classification is an \nincredibly well-studied field\n with a number of vetted solutions such as \nRabiner's pattern recognition approach\n or \nBachu's zero-crossing rate approach\n. Pitch shifting (next lab) does not require highly-accurate voiced/unvoiced detection however, so we will use a much simpler technique.\n\n\nThe energy of a signal can be a useful surrogate for voiced/unvoiced classification. Put simply, if a signal has enough energy, we assume it is voiced and continue our pitch analysis. \nThe energy of a discrete-time signal is given as follows:\n\n\n\n\n\n    E_s = \\sum_{n=-\\infty}^{\\infty} |x(n)|^2\n\n\n\n\n\n\n\nAssignment\n\n\nUsing the given test speech signal and the test code given below, determine a useful threshold for \nE_s\n and classify frames as voiced (return 1) or unvoiced (return 0). The test code will plot the results for you.\n\n\n\n\nimport\n \nnumpy\n \nas\n \nnp\n\n\nimport\n \nmatplotlib.pyplot\n \nas\n \nplt\n\n\nfrom\n \nscipy.io.wavfile\n \nimport\n \nread\n,\n \nwrite\n\n\n\nFRAME_SIZE\n \n=\n \n2048\n\n\n\ndef\n \nece420ProcessFrame\n(\nframe\n):\n\n    \nisVoiced\n \n=\n \n0\n\n\n    \n#### YOUR CODE HERE ####\n\n\n    \nreturn\n \nisVoiced\n\n\n\n\n################# GIVEN CODE BELOW #####################\n\n\n\nFs\n,\n \ndata\n \n=\n \nread\n(\n'test_vector.wav'\n)\n\n\n\nnumFrames\n \n=\n \nint\n(\nlen\n(\ndata\n)\n \n/\n \nFRAME_SIZE\n)\n\n\nframesVoiced\n \n=\n \nnp\n.\nzeros\n(\nnumFrames\n)\n\n\n\nfor\n \ni\n \nin\n \nrange\n(\nnumFrames\n):\n\n    \nframe\n \n=\n \ndata\n[\ni\n \n*\n \nFRAME_SIZE\n \n:\n \n(\ni\n \n+\n \n1\n)\n \n*\n \nFRAME_SIZE\n]\n\n    \nframesVoiced\n[\ni\n]\n \n=\n \nece420ProcessFrame\n(\nframe\n.\nastype\n(\nfloat\n))\n\n\n\nplt\n.\nfigure\n()\n\n\nplt\n.\nstem\n(\nframesVoiced\n)\n\n\nplt\n.\nshow\n()\n\n\n\n\n\n\nAutocorrelation\n\n\nAutocorrelation is the process of circularly convolving a signal with itself. That is, for a real signal, the discrete autocorrelation is given as:\n\n\n\n\n\n    R_{xx}[l] = x[n] \\circledast \\tilde{x}[-n],\n\n\n\n\n\nwhere \n\\tilde{x}[-n]\n is the complex conjugate of the time reversal of \nx[n]\n. The output \nR_{xx}[l]\n measures how self-similar a signal is if shifted by some lag \nl\n. If normalized to 1 at zero lag, this can be written equivalently as:\n\n\n\n\n\n  R_{xx}[l] = \\frac{\\sum_{n=0}^{N-1} x[n] x[n - l]}{\\sum_{n=0}^{N-1} x[n]^2}\n\n\n\n\n\nFor a periodic signal, the lag \nl\n that maximizes \nR_{xx}[l]\n indicates the frequency of the signal. In other words, the signal takes \nl\n samples before repeating itself. This algorithm, combined with some additional modifications to prevent harmonics from being detected, comprises the \nmost well-known frequency estimator for speech and music\n.\n\n\n\n\nAssignment\n\n\nCalculate and plot the autocorrelation of the test signal \ntune\n using the test code below. Indicate the lag l that maximizes \nR_{xx}[l]\n. What frequency corresponds to this lag? You may not use \nnp.correlate()\n or other such functions.\n\n\n\n\nPython test code\n\n\nimport\n \nnumpy\n \nas\n \nnp\n\n\nimport\n \nmatplotlib.pyplot\n \nas\n \nplt\n\n\n\nfs\n \n=\n \n8000\n        \n# Sampling Rate is 8000\n\n\nduration\n \n=\n \n1\n     \n# 1 sec\n\n\nt\n \n=\n \nnp\n.\nlinspace\n(\n0\n,\nduration\n,\nduration\n*\nfs\n)\n\n\nfreq\n \n=\n \n10\n        \n# Tune Frequency is 10 Hz\n\n\ntune\n \n=\n \nnp\n.\nsin\n(\n2\n*\nnp\n.\npi\n*\nfreq\n*\nt\n)\n\n\n\n# Add some Gaussian noise \n\n\ntune\n \n+=\n \nnp\n.\nrandom\n.\nnormal\n(\n0\n,\n \n0.5\n,\n \nduration\n \n*\n \nfs\n)\n\n\n\nplt\n.\nfigure\n()\n\n\nplt\n.\nplot\n()\n\n\n\n# Start a new figure for your autocorrelation plot \n\n\nplt\n.\nfigure\n()\n \n\n\n# Your code here\n\n\n\n# Only call plt.show() at the very end of the script \n\n\nplt\n.\nshow\n()",
            "title": "Prelab 4 - Autocorrelation"
        },
        {
            "location": "/lab4/prelab/#prelab-4",
            "text": "",
            "title": "Prelab 4"
        },
        {
            "location": "/lab4/prelab/#summary",
            "text": "In this prelab, you get familiarized with two common tasks in speech signal analysis: voicing determination and autocorrelation.  Submit the assignments as a jupyter notebook to your respective TAs.",
            "title": "Summary"
        },
        {
            "location": "/lab4/prelab/#downloads",
            "text": ".wav test vector",
            "title": "Downloads"
        },
        {
            "location": "/lab4/prelab/#voicedunvoiced-detector",
            "text": "Voiced/unvoiced signal classification is an  incredibly well-studied field  with a number of vetted solutions such as  Rabiner's pattern recognition approach  or  Bachu's zero-crossing rate approach . Pitch shifting (next lab) does not require highly-accurate voiced/unvoiced detection however, so we will use a much simpler technique.  The energy of a signal can be a useful surrogate for voiced/unvoiced classification. Put simply, if a signal has enough energy, we assume it is voiced and continue our pitch analysis.  The energy of a discrete-time signal is given as follows:   \n    E_s = \\sum_{n=-\\infty}^{\\infty} |x(n)|^2    Assignment  Using the given test speech signal and the test code given below, determine a useful threshold for  E_s  and classify frames as voiced (return 1) or unvoiced (return 0). The test code will plot the results for you.   import   numpy   as   np  import   matplotlib.pyplot   as   plt  from   scipy.io.wavfile   import   read ,   write  FRAME_SIZE   =   2048  def   ece420ProcessFrame ( frame ): \n     isVoiced   =   0 \n\n     #### YOUR CODE HERE #### \n\n     return   isVoiced  ################# GIVEN CODE BELOW #####################  Fs ,   data   =   read ( 'test_vector.wav' )  numFrames   =   int ( len ( data )   /   FRAME_SIZE )  framesVoiced   =   np . zeros ( numFrames )  for   i   in   range ( numFrames ): \n     frame   =   data [ i   *   FRAME_SIZE   :   ( i   +   1 )   *   FRAME_SIZE ] \n     framesVoiced [ i ]   =   ece420ProcessFrame ( frame . astype ( float ))  plt . figure ()  plt . stem ( framesVoiced )  plt . show ()",
            "title": "Voiced/Unvoiced Detector"
        },
        {
            "location": "/lab4/prelab/#autocorrelation",
            "text": "Autocorrelation is the process of circularly convolving a signal with itself. That is, for a real signal, the discrete autocorrelation is given as:   \n    R_{xx}[l] = x[n] \\circledast \\tilde{x}[-n],   where  \\tilde{x}[-n]  is the complex conjugate of the time reversal of  x[n] . The output  R_{xx}[l]  measures how self-similar a signal is if shifted by some lag  l . If normalized to 1 at zero lag, this can be written equivalently as:   \n  R_{xx}[l] = \\frac{\\sum_{n=0}^{N-1} x[n] x[n - l]}{\\sum_{n=0}^{N-1} x[n]^2}   For a periodic signal, the lag  l  that maximizes  R_{xx}[l]  indicates the frequency of the signal. In other words, the signal takes  l  samples before repeating itself. This algorithm, combined with some additional modifications to prevent harmonics from being detected, comprises the  most well-known frequency estimator for speech and music .   Assignment  Calculate and plot the autocorrelation of the test signal  tune  using the test code below. Indicate the lag l that maximizes  R_{xx}[l] . What frequency corresponds to this lag? You may not use  np.correlate()  or other such functions.",
            "title": "Autocorrelation"
        },
        {
            "location": "/lab4/prelab/#python-test-code",
            "text": "import   numpy   as   np  import   matplotlib.pyplot   as   plt  fs   =   8000          # Sampling Rate is 8000  duration   =   1       # 1 sec  t   =   np . linspace ( 0 , duration , duration * fs )  freq   =   10          # Tune Frequency is 10 Hz  tune   =   np . sin ( 2 * np . pi * freq * t )  # Add some Gaussian noise   tune   +=   np . random . normal ( 0 ,   0.5 ,   duration   *   fs )  plt . figure ()  plt . plot ()  # Start a new figure for your autocorrelation plot   plt . figure ()   # Your code here  # Only call plt.show() at the very end of the script   plt . show ()",
            "title": "Python test code"
        },
        {
            "location": "/lab5/prelab/",
            "text": "Prelab 5 - Resampling\n\n\nSummary\n\n\nWhen Cher\u2019s song \n\"Believe\"\n hit shelves on October 22, 1998, music changed forever. Cher\u2019s single sold 11 million copies worldwide, earned her a Grammy Award, and topped the charts in 23 countries. Auto-Tune is the algorithm behind the success of \"Believe\". It was created by an ECE Illinois PhD alum, Dr. Andy Hildebrand. He came up with the idea of using autocorrelation to do pitch correction while working as an oil engineer at Exxon. We will explore a variant of Auto-Tune in more detail in the lab.\n\n\nIn this prelab, we will try a naive pitch-shifting method: lowering the pitch by a factor of 3 and increasing the pitch by a factor of 2 using resampling.\n\n\n\n\n\nSubmit the assignment as a Jupyter Notebook to your respective TA.\n\n\nDownloads\n\n\n\n\ntest_audio.wav\n\n\n\n\nPart 1 - Upsample by 3x\n\n\nUpsampling a signal by a factor of 3 means inserting 2 zeros between each sample. For example, we insert zeros in this 20-sample frame:\n\n\n\n\nand get a 60-sample signal:\n\n\n\n\n\n\nAssignment\n\n\nLoad the given test vector and upsample by 3x. Plot the FFT of the original signal and the zero-inserted signal. \n\n\n\n\nWhat is the relationship between the original signal's FFT and the upsampled signal's FFT?\n\n\nHow do we preserve the original information without introducing aliasing? \n\n\n\n\n\n\nPart 2 - Downsample by 2x\n\n\nDownsampling a signal by a factor of 2 means removing every other sample. When played back at the same sampling rate, this results in a signal that sounds higher-pitched\n\n\n\n\nAssignment\n\n\nLoad the given test vector and downsample by 2x. Plot the FFT of the original signal and the downsampled signal. \n\n\n\n\nWhat is the relationship between the original signal's FFT and the downsampled signal's FFT?\n\n\nHow do we preserve the original information without introducing aliasing? \n\n\n\n\n\n\nPart 3 - Playback\n\n\nParts 1 and 2 (along with a couple other components) can be chained together to implement a resampler of any ratio \n\\frac{M}{N}\n. To get some intuition for the output of a resampled signal, we will use SciPy's resampling function \nscipy.signal.resample_poly\n. This can be called with the following code:\n\n\nfrom\n \nscipy\n \nimport\n \nsignal\n\n\nfrom\n \nscipy.io.wavfile\n \nimport\n \nread\n\n\nfrom\n \nIPython.display\n \nimport\n \nAudio\n\n\n\nFs\n,\n \ndata\n \n=\n \nread\n(\n'test_audio.wav'\n)\n\n\ndata\n \n=\n \ndata\n[:,\n \n0\n]\n\n\n\nup_ratio\n \n=\n \n??\n\n\ndown_ratio\n \n=\n \n??\n\n\n\noutput\n \n=\n \nsignal\n.\nresample_poly\n(\ndata\n,\n \nup_ratio\n,\n \ndown_ratio\n)\n\n\n\nAudio\n(\noutput\n,\n \nrate\n=\nFs\n)\n\n\n\n\n\n\n\n\nAssignment\n\n\nTry a few resampling ratios, including at least \n\\frac{M}{N} = \\left\\{2, 1, \\frac{1}{2}\\right\\}\n. What does the resulting audio sound like? Be specific in your response. How do the vocal characteristics of the singer change?",
            "title": "Prelab 5 - Resampling"
        },
        {
            "location": "/lab5/prelab/#prelab-5-resampling",
            "text": "",
            "title": "Prelab 5 - Resampling"
        },
        {
            "location": "/lab5/prelab/#summary",
            "text": "When Cher\u2019s song  \"Believe\"  hit shelves on October 22, 1998, music changed forever. Cher\u2019s single sold 11 million copies worldwide, earned her a Grammy Award, and topped the charts in 23 countries. Auto-Tune is the algorithm behind the success of \"Believe\". It was created by an ECE Illinois PhD alum, Dr. Andy Hildebrand. He came up with the idea of using autocorrelation to do pitch correction while working as an oil engineer at Exxon. We will explore a variant of Auto-Tune in more detail in the lab.  In this prelab, we will try a naive pitch-shifting method: lowering the pitch by a factor of 3 and increasing the pitch by a factor of 2 using resampling.   Submit the assignment as a Jupyter Notebook to your respective TA.",
            "title": "Summary"
        },
        {
            "location": "/lab5/prelab/#downloads",
            "text": "test_audio.wav",
            "title": "Downloads"
        },
        {
            "location": "/lab5/prelab/#part-1-upsample-by-3x",
            "text": "Upsampling a signal by a factor of 3 means inserting 2 zeros between each sample. For example, we insert zeros in this 20-sample frame:   and get a 60-sample signal:    Assignment  Load the given test vector and upsample by 3x. Plot the FFT of the original signal and the zero-inserted signal.    What is the relationship between the original signal's FFT and the upsampled signal's FFT?  How do we preserve the original information without introducing aliasing?",
            "title": "Part 1 - Upsample by 3x"
        },
        {
            "location": "/lab5/prelab/#part-2-downsample-by-2x",
            "text": "Downsampling a signal by a factor of 2 means removing every other sample. When played back at the same sampling rate, this results in a signal that sounds higher-pitched   Assignment  Load the given test vector and downsample by 2x. Plot the FFT of the original signal and the downsampled signal.    What is the relationship between the original signal's FFT and the downsampled signal's FFT?  How do we preserve the original information without introducing aliasing?",
            "title": "Part 2 - Downsample by 2x"
        },
        {
            "location": "/lab5/prelab/#part-3-playback",
            "text": "Parts 1 and 2 (along with a couple other components) can be chained together to implement a resampler of any ratio  \\frac{M}{N} . To get some intuition for the output of a resampled signal, we will use SciPy's resampling function  scipy.signal.resample_poly . This can be called with the following code:  from   scipy   import   signal  from   scipy.io.wavfile   import   read  from   IPython.display   import   Audio  Fs ,   data   =   read ( 'test_audio.wav' )  data   =   data [:,   0 ]  up_ratio   =   ??  down_ratio   =   ??  output   =   signal . resample_poly ( data ,   up_ratio ,   down_ratio )  Audio ( output ,   rate = Fs )    Assignment  Try a few resampling ratios, including at least  \\frac{M}{N} = \\left\\{2, 1, \\frac{1}{2}\\right\\} . What does the resulting audio sound like? Be specific in your response. How do the vocal characteristics of the singer change?",
            "title": "Part 3 - Playback"
        },
        {
            "location": "/lab1/lab/",
            "text": "Lab 1\n\n\nSummary\n\n\nIn this lab, you get familiarized with PyCharm and Android Studio development environment. Using a simple step detector as an example, you will write and debug your code in Python first, and later in Android.\n\n\nDownloads\n\n\nAndroid Project source code\n\n\nPython\n\n\nDownload \nPyCharm\n to your computer. If you are on your personal computer, you might apply for the \nEducational license\n. Install \nAnaconda\n Python distribution. The Python 3.5 distribution is used in this tutorial.\n\n\nSet up Python Interpreter in PyCharm.\n\n\n\n\n\n\nOpen PyCharm. Go to PyCharm Preferences\n\n\n\n\n\n\nGo to Project Interpreter. Navigate to Anaconda python path on your computer: \n/anaconda/bin/python\n on Mac or \nC:\\Users\\YourUserName\\anaconda\\bin\\python\n on Windows. Click OK.\n\n\n\n\n\n\nCreate a new Project in PyCharm.\n\n\n\n\n\n\nGo to File -> New Project or Create New Project from the PyCharm Welcome Screen.\n\n\n\n\n\n\nPick a Pure Python project. Make sure the Interpreter is set to the Anaconda distribution.\n\n\n\n\n\n\nName your project \nlab1\n (or a name you prefer). Click OK.\n\n\n\n\n\n\nRight click the lab1 Navigation bar. Choose New -> Python File. Name it \npeak_detection.py\n. Click OK.\n\n\n\n\n\n\n\n\nPlot the sensor data\n\n\nWe generated sample sensor data in the file  \nsample_sensor_data.csv\n.\nCopy \nsample_sensor_data.csv\n to your lab1 folder.\n\n\nIn the file \npeak_detection.py\n, type the following code (you may ignore line number 5):\n\n\n\n\nWe imported the necessary libraries (\nnumpy\n, \nmatplotlib\n and \nos\n) from lines 1 to 3. If your csv file is not in the lab1 folder, you can use \nos.chdir()\n to navigate to the correct path like in line 5.\n\n\nThe accelerometer and gyroscope data are extracted in line 11 and 12. The time vector is extracted in line 9. Let's plot the first axis of accelerometer data in line 14 and 15.\n\n\nRun your program\n\n\nRight click anywhere inside the code page and choose \nRun 'peak_detection'\n.\n\n\n\n\nThe plot is as follows:\n\n\n\n\nClose the figure to stop the program from running.\n\n\n\n\nAssignment 1\n\n\nGive your plot a title of \"First axis of accelerometer data\". Name your x-axis \"Time\" and your y-axis \"Meters per second.\n\n\nShow the TA when done.\n\n\n\n\nDebug your code\n\n\nClick on the space between the code the the line number on line 14 to set a break point (red circle).\n\n\n\n\nRight click inside the \npeak_detection.py\n or click on the Debug button on the top right corner of PyCharm (the bug symbol). The execution will stop at line 14.\n\n\nIn the Debug console, you can view all the variables currently created. If it is a numpy array, you can click on View as Array (at the end of the row) to view it as a table.\n \n\n\nDebug in the IPython Console\n\n\nWhile in the Debug mode, you can write code on the fly to test out your logic. Switch to the Console tab and choose Show Python Prompt (where the red arrow points to)\n\n\n\n\nThis is the python interactive console connected to your debugger. For example, let's find the maximum value of the accelerometer data in the console. Type \naccel_data[0].max()\n  in the console and press enter. It should give a value of \n12.507\n.\n\n\nMore info on how to step through your program \nhere\n.\n\n\nDefine your function\n\n\nNow let's define a \nfunction\n to find a peak in your signal.\nGo to the top of the function and define a function peak_detection on line 4. Please type the following lines:\n\n\n\n\nThe function accepts the time array \nt\n and the accelerometer data array \nsig\n.\n\n\nWe define a Python \nlist\n of peaks on line 5. For now we detect a single peak, the maximum value of our signal. We use a for loop (line 8) to traverse through the our array \nsig\n. Initially, the \nmax_val\n is set to minus infinity (why?). Every time we encounter a greater value than \nmax_val\n (line 9), we record this value and its time position (line 10 and 11).\n\n\n\n\nTip\n\n\nPython relies on \nindentation\n to differentiate different parts of the code.\n\n\n\n\nThe position and the maximum value are appended to the list of \npeaks\n in line 13. In line 14, we turn our list to numpy array for plotting and return it to the calling function.\n\n\nNow that we define our function, we can call it and plot our peak.\nAdd this line before the plot function:\n\n\nmax_peaks\n \n=\n \npeak_detection\n(\ntimestamps\n,\n \naccel_data\n[\n0\n])\n\n\n\n\n\n\nAdd this line of code after the plot function (line 14).\n\n\nplt\n.\nscatter\n(\nmax_peaks\n[:,\n0\n],\n \nmax_peaks\n[:,\n1\n],\n \ncolor\n \n=\n \n'red'\n)\n\n\n\n\n\n\n\n\nQuestion\n\n\nWhat is the dimension of max_peaks?\n\n\n\n\nNow run or debug your program. You should see a red peak overlaid onto the signal.\n\n\n\n\n\n\nAssignment 2\n\n\nModify the \npeak_detection\n function to detect other peaks in the signal. Add the new peaks and time positions to our \npeaks\n list. The function should accept a new input parameter \nthresh\n (peaks should have value greater than \nthresh\n).\n\n\nRename your python file to \nfirstname_lastName.py\n when you submit it.\n\n\nHere is a sample output:\n\n\n\n\n\n\nAndroid\n\n\nDownload \nAndroid Studio\n. On the Welcome Screen, click Configure -> SDK Manager.\n\n\n\n\nSwitch to tab SDK tools, and check the boxes as in the figure below. These will install the requisite Java SDK, C++ NDK, and debugging tools. Click OK to install.\n\n\n\n\nChoose Open an existing Android Project. Navigate to the lab1 project. It might take a few seconds for Android Studio to index the files and configurations.\n\n\n.\n\n\nExplore the Project\n\n\nChoose the Android tab from the Project Sidebar. Expand the \njava\n and \nres\n folder. Double click on the \nactivity_pedometer_simple.xml\n in the \nres/layout\n folder.\n\n\n\n\nOur user interface consists of a label which says \"Hello world!\" and a \nGraphView\n to display our pedometer signal. Click on the text label, in the popped out Properties tool bar, change the text from \"Hello World!\" to \"Steps Detected\". Save your changes.\n\n\n\n\nDrag two buttons from the Widgets in the Palette View on the left side into our \nPedometerSimple\n layout.\n\n\n\n\nSwitch to the Text view (at the bottom left, next to Design tab) to edit the layout of our user interface in xml.\n\n\n\n\nThe above xml view is before the buttons are added.\n\n\n\n\n\n\nRight click on the line \nandroid:id=\"@+id/button\"\n and choose Refactor->Rename, and rename it to \nbuttonStart\n. Notice that all of the references to button change to \nbuttonStart\n.\n\n\n\n\n\n\nChange the text of the Buttons to \"Start!\" and \"Stop!\" (in the android:text properties). And change the \nandroid:id\n of the Stop Button to \nbuttonStop\n.\n\n\n\n\n\n\nSave the xml file and switch back the Design View to see your new design.\n\n\n\n\n\n\nButton event listener\n\n\nNavigate to \njava\n folder. Expand \ncom.ece420.lab1\n and open \nPedometerSimple.java\n.\n\n\nA \nTextView\n named \ntextStatus\n and a \nButton\n named \nbuttonStart\n are declared in line 25 and 26. We connect those two variables to the user interface components in line 43 to 44.\n\n\nWhen you press the Start Button, the code from line 49 to 57 will run. These are event handlers for the button click event. We start our data collection using the \nSensorReader.startCollection()\n function and change the label \ntextStatus\n to \n\"Started\"\n.\n\n\n\n\nAssignment 3\n\n\nCreate a variable named \nbuttonStop\n (of class \nButton\n) and connect this variable to the \nbuttonStop\n UI element by using the function \nfindViewById\n.\n\n\nUnder the \nbuttonStart.setOnClickListener()\n event handler, write the event handler for the \nbuttonStop\n click event as follows:\n\n\nIf the \nsensorsOn\n variable is \ntrue\n, set it to \nfalse\n and call the \nStopCollection()\n function on the \nmSensorReader\n class instance. Also set the \ntextStatus\n label to \"\nStopped!\"\n.\n\n\nQuestion\n : What do \nonResume()\n and \nonPause()\n function do?\n\n\nPlease ask your TA to check your answers before continuing the next part.\n\n\n\n\nPlotting the sensor data\n\n\nLine 78 to 86 is where we connect our Plotting Control (GraphView) in the user interface to our java code. (your line number might change. See the figure below)\n\n\n\n\nSince Android 6.0, Android apps request permissions when they are running, not when they are installed. In our code, we asked for permissions to read and write to the external storage. (line 89 to line 99).\n\n\n\n\nDeveloper mode on the SHIELD tablet.\n\n\nFrom the home screen of your tablet, swipe down twice and choose the Settings icon. Scroll down to the About Tablet setting. Inside the About Tablet setting, find the Build Number item. Tap \n7\n times, and there will be notifications informing that you are now a developer.\n\n\nGo back to the main Settings menu, and choose the Developer options. Turn on the USB debugging.\nConnect your tablet to your computer via USB. Say yes to the \nrsa-fingerprint\n notification.\n\n\nIf you are on Windows, you may need to install the OEM USB Drivers for your device. See \nthis page\n for help if your device is not automatically recognized.\n\n\nRun the step detector app\n\n\nGo to the \nRun\n menu and choose \nRun app\n. This will handle building the Android target and uploading the APK to the device.\n\n\nChoose the \nNVIDIA SHIELD TABLET\n when the Select Deployment Target screen appears. Your application will display a continuous stream of sensor data and the estimated number of steps based on the accelerometer readings.\n\n\n\n\n\n\n\n\n\nDebugging your app.\n\n\nOpen \nStepDetector.java\n. Set a breakpoint at line 53 (click on the space next to the line number). In the \nRun\n menu, choose \nDebug app\n (or choose the Debug app symbol in the toolbar). Notice the similarity between PyCharm and Android Studio debugging mode.\n\n\nOnce your app shows up, press the Start button on the tablet to begin recording data.\n\n\n\n\nAndroid Studio has multiple ways to step through code. For now, click \nStep Over\n to move the code to the next line.\n\n\nSwitch to the Android Monitor tab. You can see the output of the \nLog.d()\n function in this window. Android's \nLog function\n is a useful tool for debugging and monitoring your app.\n\n\nIn the Android Monitor view, type \nSTEP_DETECTOR\n (the \nTAG\n that we used in the \nLog\n function) in the search field and press Enter to filter out other \nLog\n outputs.\n\n\nGo back to the Debug mode, remove the breakpoint (click on the red circle) and click \nResume\n (the play button in the left column).\n\n\nNow switch back to the Android Monitor Window. Observe that whenever the code detects a peak, the Android Monitor shows the output of our \nLog\n function, which is every time the accelerometer value is greater than \nACCEL_THRESHOLD\n (line 52).\n\n\nUnderstanding the StepDetector class\n\n\nThe \nStepDetector\n class detects peaks for an online stream of data. It stores the accelerometer values in the \naccelBuffer\n list. The buffer has a size of \nN_SAMPLES = 11\n.\n\n\n\n\nQuestion\n\n\nWhat does \ntimeSinceLastStep\n do?\n\n\n\n\nTurn in your assignments\n\n\nPlease send your Python peak detection code (Assignment 2) to all of the TAs.\nName your file \nfirstname_lastName.py\n. These should be submitted individually.",
            "title": "Lab 1 - IMU Pedometer"
        },
        {
            "location": "/lab1/lab/#lab-1",
            "text": "",
            "title": "Lab 1"
        },
        {
            "location": "/lab1/lab/#summary",
            "text": "In this lab, you get familiarized with PyCharm and Android Studio development environment. Using a simple step detector as an example, you will write and debug your code in Python first, and later in Android.",
            "title": "Summary"
        },
        {
            "location": "/lab1/lab/#downloads",
            "text": "Android Project source code",
            "title": "Downloads"
        },
        {
            "location": "/lab1/lab/#python",
            "text": "Download  PyCharm  to your computer. If you are on your personal computer, you might apply for the  Educational license . Install  Anaconda  Python distribution. The Python 3.5 distribution is used in this tutorial.",
            "title": "Python"
        },
        {
            "location": "/lab1/lab/#set-up-python-interpreter-in-pycharm",
            "text": "Open PyCharm. Go to PyCharm Preferences    Go to Project Interpreter. Navigate to Anaconda python path on your computer:  /anaconda/bin/python  on Mac or  C:\\Users\\YourUserName\\anaconda\\bin\\python  on Windows. Click OK.",
            "title": "Set up Python Interpreter in PyCharm."
        },
        {
            "location": "/lab1/lab/#create-a-new-project-in-pycharm",
            "text": "Go to File -> New Project or Create New Project from the PyCharm Welcome Screen.    Pick a Pure Python project. Make sure the Interpreter is set to the Anaconda distribution.    Name your project  lab1  (or a name you prefer). Click OK.    Right click the lab1 Navigation bar. Choose New -> Python File. Name it  peak_detection.py . Click OK.",
            "title": "Create a new Project in PyCharm."
        },
        {
            "location": "/lab1/lab/#plot-the-sensor-data",
            "text": "We generated sample sensor data in the file   sample_sensor_data.csv .\nCopy  sample_sensor_data.csv  to your lab1 folder.  In the file  peak_detection.py , type the following code (you may ignore line number 5):   We imported the necessary libraries ( numpy ,  matplotlib  and  os ) from lines 1 to 3. If your csv file is not in the lab1 folder, you can use  os.chdir()  to navigate to the correct path like in line 5.  The accelerometer and gyroscope data are extracted in line 11 and 12. The time vector is extracted in line 9. Let's plot the first axis of accelerometer data in line 14 and 15.",
            "title": "Plot the sensor data"
        },
        {
            "location": "/lab1/lab/#run-your-program",
            "text": "Right click anywhere inside the code page and choose  Run 'peak_detection' .   The plot is as follows:   Close the figure to stop the program from running.   Assignment 1  Give your plot a title of \"First axis of accelerometer data\". Name your x-axis \"Time\" and your y-axis \"Meters per second.  Show the TA when done.",
            "title": "Run your program"
        },
        {
            "location": "/lab1/lab/#debug-your-code",
            "text": "Click on the space between the code the the line number on line 14 to set a break point (red circle).   Right click inside the  peak_detection.py  or click on the Debug button on the top right corner of PyCharm (the bug symbol). The execution will stop at line 14.  In the Debug console, you can view all the variables currently created. If it is a numpy array, you can click on View as Array (at the end of the row) to view it as a table.",
            "title": "Debug your code"
        },
        {
            "location": "/lab1/lab/#debug-in-the-ipython-console",
            "text": "While in the Debug mode, you can write code on the fly to test out your logic. Switch to the Console tab and choose Show Python Prompt (where the red arrow points to)   This is the python interactive console connected to your debugger. For example, let's find the maximum value of the accelerometer data in the console. Type  accel_data[0].max()   in the console and press enter. It should give a value of  12.507 .  More info on how to step through your program  here .",
            "title": "Debug in the IPython Console"
        },
        {
            "location": "/lab1/lab/#define-your-function",
            "text": "Now let's define a  function  to find a peak in your signal.\nGo to the top of the function and define a function peak_detection on line 4. Please type the following lines:   The function accepts the time array  t  and the accelerometer data array  sig .  We define a Python  list  of peaks on line 5. For now we detect a single peak, the maximum value of our signal. We use a for loop (line 8) to traverse through the our array  sig . Initially, the  max_val  is set to minus infinity (why?). Every time we encounter a greater value than  max_val  (line 9), we record this value and its time position (line 10 and 11).   Tip  Python relies on  indentation  to differentiate different parts of the code.   The position and the maximum value are appended to the list of  peaks  in line 13. In line 14, we turn our list to numpy array for plotting and return it to the calling function.  Now that we define our function, we can call it and plot our peak.\nAdd this line before the plot function:  max_peaks   =   peak_detection ( timestamps ,   accel_data [ 0 ])   Add this line of code after the plot function (line 14).  plt . scatter ( max_peaks [:, 0 ],   max_peaks [:, 1 ],   color   =   'red' )    Question  What is the dimension of max_peaks?   Now run or debug your program. You should see a red peak overlaid onto the signal.    Assignment 2  Modify the  peak_detection  function to detect other peaks in the signal. Add the new peaks and time positions to our  peaks  list. The function should accept a new input parameter  thresh  (peaks should have value greater than  thresh ).  Rename your python file to  firstname_lastName.py  when you submit it.  Here is a sample output:",
            "title": "Define your function"
        },
        {
            "location": "/lab1/lab/#android",
            "text": "Download  Android Studio . On the Welcome Screen, click Configure -> SDK Manager.   Switch to tab SDK tools, and check the boxes as in the figure below. These will install the requisite Java SDK, C++ NDK, and debugging tools. Click OK to install.   Choose Open an existing Android Project. Navigate to the lab1 project. It might take a few seconds for Android Studio to index the files and configurations.  .",
            "title": "Android"
        },
        {
            "location": "/lab1/lab/#explore-the-project",
            "text": "Choose the Android tab from the Project Sidebar. Expand the  java  and  res  folder. Double click on the  activity_pedometer_simple.xml  in the  res/layout  folder.   Our user interface consists of a label which says \"Hello world!\" and a  GraphView  to display our pedometer signal. Click on the text label, in the popped out Properties tool bar, change the text from \"Hello World!\" to \"Steps Detected\". Save your changes.   Drag two buttons from the Widgets in the Palette View on the left side into our  PedometerSimple  layout.   Switch to the Text view (at the bottom left, next to Design tab) to edit the layout of our user interface in xml.   The above xml view is before the buttons are added.    Right click on the line  android:id=\"@+id/button\"  and choose Refactor->Rename, and rename it to  buttonStart . Notice that all of the references to button change to  buttonStart .    Change the text of the Buttons to \"Start!\" and \"Stop!\" (in the android:text properties). And change the  android:id  of the Stop Button to  buttonStop .    Save the xml file and switch back the Design View to see your new design.",
            "title": "Explore the Project"
        },
        {
            "location": "/lab1/lab/#button-event-listener",
            "text": "Navigate to  java  folder. Expand  com.ece420.lab1  and open  PedometerSimple.java .  A  TextView  named  textStatus  and a  Button  named  buttonStart  are declared in line 25 and 26. We connect those two variables to the user interface components in line 43 to 44.  When you press the Start Button, the code from line 49 to 57 will run. These are event handlers for the button click event. We start our data collection using the  SensorReader.startCollection()  function and change the label  textStatus  to  \"Started\" .   Assignment 3  Create a variable named  buttonStop  (of class  Button ) and connect this variable to the  buttonStop  UI element by using the function  findViewById .  Under the  buttonStart.setOnClickListener()  event handler, write the event handler for the  buttonStop  click event as follows:  If the  sensorsOn  variable is  true , set it to  false  and call the  StopCollection()  function on the  mSensorReader  class instance. Also set the  textStatus  label to \" Stopped!\" .  Question  : What do  onResume()  and  onPause()  function do?  Please ask your TA to check your answers before continuing the next part.",
            "title": "Button event listener"
        },
        {
            "location": "/lab1/lab/#plotting-the-sensor-data",
            "text": "Line 78 to 86 is where we connect our Plotting Control (GraphView) in the user interface to our java code. (your line number might change. See the figure below)   Since Android 6.0, Android apps request permissions when they are running, not when they are installed. In our code, we asked for permissions to read and write to the external storage. (line 89 to line 99).",
            "title": "Plotting the sensor data"
        },
        {
            "location": "/lab1/lab/#developer-mode-on-the-shield-tablet",
            "text": "From the home screen of your tablet, swipe down twice and choose the Settings icon. Scroll down to the About Tablet setting. Inside the About Tablet setting, find the Build Number item. Tap  7  times, and there will be notifications informing that you are now a developer.  Go back to the main Settings menu, and choose the Developer options. Turn on the USB debugging.\nConnect your tablet to your computer via USB. Say yes to the  rsa-fingerprint  notification.  If you are on Windows, you may need to install the OEM USB Drivers for your device. See  this page  for help if your device is not automatically recognized.",
            "title": "Developer mode on the SHIELD tablet."
        },
        {
            "location": "/lab1/lab/#run-the-step-detector-app",
            "text": "Go to the  Run  menu and choose  Run app . This will handle building the Android target and uploading the APK to the device.  Choose the  NVIDIA SHIELD TABLET  when the Select Deployment Target screen appears. Your application will display a continuous stream of sensor data and the estimated number of steps based on the accelerometer readings.",
            "title": "Run the step detector app"
        },
        {
            "location": "/lab1/lab/#debugging-your-app",
            "text": "Open  StepDetector.java . Set a breakpoint at line 53 (click on the space next to the line number). In the  Run  menu, choose  Debug app  (or choose the Debug app symbol in the toolbar). Notice the similarity between PyCharm and Android Studio debugging mode.  Once your app shows up, press the Start button on the tablet to begin recording data.   Android Studio has multiple ways to step through code. For now, click  Step Over  to move the code to the next line.  Switch to the Android Monitor tab. You can see the output of the  Log.d()  function in this window. Android's  Log function  is a useful tool for debugging and monitoring your app.  In the Android Monitor view, type  STEP_DETECTOR  (the  TAG  that we used in the  Log  function) in the search field and press Enter to filter out other  Log  outputs.  Go back to the Debug mode, remove the breakpoint (click on the red circle) and click  Resume  (the play button in the left column).  Now switch back to the Android Monitor Window. Observe that whenever the code detects a peak, the Android Monitor shows the output of our  Log  function, which is every time the accelerometer value is greater than  ACCEL_THRESHOLD  (line 52).",
            "title": "Debugging your app."
        },
        {
            "location": "/lab1/lab/#understanding-the-stepdetector-class",
            "text": "The  StepDetector  class detects peaks for an online stream of data. It stores the accelerometer values in the  accelBuffer  list. The buffer has a size of  N_SAMPLES = 11 .   Question  What does  timeSinceLastStep  do?",
            "title": "Understanding the StepDetector class"
        },
        {
            "location": "/lab1/lab/#turn-in-your-assignments",
            "text": "Please send your Python peak detection code (Assignment 2) to all of the TAs.\nName your file  firstname_lastName.py . These should be submitted individually.",
            "title": "Turn in your assignments"
        },
        {
            "location": "/lab2/lab/",
            "text": "Lab 2 - Audio Filtering\n\n\nSummary\n\n\nIn this lab, you will learn how to design FIR notch filters to specification using Python and how to implement filters efficiently in Android.\n\n\nDownloads\n\n\nAndroid Project source code\n\n\nPython\n\n\n\n\n\nPart 1 - Filter Generation\n\n\nFilter design in Python is very similar to filter design in MATLAB. SciPy has a \nSignal Processing library\n that contains built-in code for convolution, spline interpolation, filtering and filter design, as well as other signal analysis. If you are looking for a DSP function that exists in MATLAB, this library should be the first place you check.\n\n\nIn particular, we will be looking at \nscipy.signal.firls\n for our FIR filter design. Your task will be to generate the coefficients for a bandstop filter with the given specifications:\n\n\n\n\nFrequencies between 1KHz and 2KHz should be attenuated to approximately -20dB or less.\n\n\nAll other frequencies should pass through with approximately unity gain.\n\n\nAssume a sampling rate of 48kHz.\n\n\n\n\nOur final goal with this filter will be to allow speech through with no attenuation but suppress pure sine tones between 1KHz and 2KHz. Some things to think about:\n\n\n\n\nYou can design any filter if you allow the filter order to go to infinity. What are the practical considerations to using a longer filter?\n\n\nThe sharper the transition bands, the larger the ripple in the passband. We've defined a relatively narrow stopband. How wide can you make the transition bands while still meeting your application's requirements?\n\n\n\n\n\n\nAssignment\n\n\nGenerate an FIR filter using \nscipy.signal.firls()\n. Sample code start you off and display the frequency response is given below.\n\n\nShow your TA when your filter design is done. Defend your design decisions regarding points 1 and 2 from above. \n\n\n\n\nSkeleton Python code shown below. Feel free to modify it as you see fit. \n\n\nimport\n \nnumpy\n \nas\n \nnp\n\n\nimport\n \nmatplotlib.pyplot\n \nas\n \nplt\n\n\nfrom\n \nscipy\n \nimport\n \nsignal\n\n\n\n# Your filter design here\n\n\n# firls() can be called via signal.firls()\n\n\nb\n \n=\n \n??\n\n\n\n# Signal analysis\n\n\nw\n,\n \nh\n \n=\n \nsignal\n.\nfreqz\n(\nb\n)\n\n\n\nplt\n.\nfigure\n()\n\n\nplt\n.\nsubplot\n(\n2\n,\n1\n,\n1\n)\n\n\nplt\n.\ntitle\n(\n'Digital filter frequency response, N = '\n \n+\n \nstr\n(\nlen\n(\nb\n)))\n\n\nplt\n.\nplot\n(\nw\n \n/\n \nnp\n.\npi\n,\n \n20\n \n*\n \nnp\n.\nlog10\n(\nabs\n(\nh\n)),\n \n'b'\n)\n\n\nplt\n.\nylabel\n(\n'Amplitude [dB]'\n,\n \ncolor\n=\n'b'\n)\n\n\nplt\n.\ngrid\n()\n\n\nplt\n.\naxis\n(\n'tight'\n)\n\n\n\nplt\n.\nsubplot\n(\n2\n,\n1\n,\n2\n)\n\n\nangles\n \n=\n \nnp\n.\nunwrap\n(\nnp\n.\nangle\n(\nh\n))\n\n\nplt\n.\nplot\n(\nw\n \n/\n \nnp\n.\npi\n,\n \nangles\n,\n \n'g'\n)\n\n\nplt\n.\nylabel\n(\n'Angle (radians)'\n,\n \ncolor\n=\n'g'\n)\n\n\nplt\n.\ngrid\n()\n\n\nplt\n.\naxis\n(\n'tight'\n)\n\n\nplt\n.\nxlabel\n(\n'Frequency [0 to Nyquist Hz, normalized]'\n)\n\n\nplt\n.\nshow\n()\n\n\n\n\n\n\nPart 2 - FIR Filter\n\n\nNow that you have your filter coefficients, you will implement your filter in Python and test it on sample data. This prototyping stage is to get you comfortable with actually using a filter before implementing it on Android, which is significantly more difficult to debug. Your filtering code should only be a few lines long. Do not use any \nscipy.signal\n functions such as \nlfilt\n to compute the filtered output. \n\n\n\n\nAssignment\n\n\nImplement the filter designed in Part 1. Test your filter on the time_domain signal \ntest_data\n given below and plot the time domain result.\n\n\nShow your TA when you are done.\n\n\n\n\nPython test code:\n\n\nimport\n \nnumpy\n \nas\n \nnp\n \n\nimport\n \nmatplotlib.pyplot\n \nas\n \nplt\n \n\nfrom\n \nscipy\n \nimport\n \nsignal\n \n\n\nF_s\n \n=\n \n48000\n\n\nt\n \n=\n \n[\ni\n \n/\n \nF_s\n \nfor\n \ni\n \nin\n \nrange\n(\n2\n \n*\n \nF_s\n)]\n\n\n\ntest_data\n \n=\n \nsignal\n.\nchirp\n(\nt\n,\n \n1\n,\n \nt\n[\n-\n1\n],\n \n24000\n,\n \nmethod\n=\n'logarithmic'\n)\n\n\n\n# ... filter ...\n\n\n\n\n\n\nThe following code may be used to convert a Python array of coefficients to a static C++ array initialization:\n\n\na\n \n=\n \n[\n1\n,\n \n2\n,\n \n3\n,\n \n4\n,\n \n5\n,\n \n6\n]\n\n\n\ncoef_str\n \n=\n \n\"float coefs[] = {\"\n\n\n\nfor\n \nval\n \nin\n \na\n:\n\n    \ncoef_str\n \n+=\n \nstr\n(\nval\n)\n \n+\n \n\", \"\n\n\n\ncoef_str\n \n=\n \ncoef_str\n[:\n-\n2\n]\n\n\ncoef_str\n \n+=\n \n\"};\"\n\n\n\nprint\n(\ncoef_str\n)\n\n\n\n\n\n\nAndroid\n\n\nPart 3 - OpenSL ES\n\n\nBackground\n\n\nIf you've worked with Android before, you may wonder why we're using the NDK for our audio transactions. An example of Java's audio recording callflow taken from \nthis StackOverflow post\n is given below:\n\n\nprivate\n \nvoid\n \nstartRecording\n()\n \n{\n\n    \nrecorder\n \n=\n \nnew\n \nAudioRecord\n(\nMediaRecorder\n.\nAudioSource\n.\nMIC\n,\n\n            \nRECORDER_SAMPLERATE\n,\n \nRECORDER_CHANNELS\n,\n\n            \nRECORDER_AUDIO_ENCODING\n,\n \nBufferElements2Rec\n \n*\n \nBytesPerElement\n);\n\n\n    \nrecorder\n.\nstartRecording\n();\n\n    \nisRecording\n \n=\n \ntrue\n;\n\n    \nrecordingThread\n \n=\n \nnew\n \nThread\n(\nnew\n \nRunnable\n()\n \n{\n\n        \npublic\n \nvoid\n \nrun\n()\n \n{\n\n            \nwriteAudioDataToFile\n();\n\n        \n}\n\n    \n},\n \n\"AudioRecorder Thread\"\n);\n\n    \nrecordingThread\n.\nstart\n();\n\n\n}\n\n\n\n\n\n\nIn the code above, we first initialize a new \nAudioRecord\n instance containing all of our recording parameters, such as sampling rate, mono or stereo, encoding format, and number of elements in the audio buffer. We tell the \nAudioRecord\n instance to begin sampling the microphone and storing data in the buffer, then we launch a thread to monitor the buffer and extract samples when it is full (not shown). \n\n\nThe problem with this approach is the \nlatency\n. Android has a longstanding problem implementing \nlow-latency audio solutions compared to iOS\n. As of Q1 2015, this was reflected in the \ndisproportionate number of music apps available for Android versus iOS\n. At a high level, Android requires large sampling buffers to be filled before the data can be read. Compare this to the last lab -- it would be as if you could only read sensor data every \nN\n samples versus every sample. For the Java implementation given above, the minimum recording and playback buffer sizes result in about 200ms round-trip delay (RTD).\n\n\nAndroid has made great strides in low-latency audio in the last few years, however. They have introduced a \nlow-latency pipeline accessible in the NDK\n that operates on the \nOpenSL ES framework\n, a standardized embedded audio API. If you use the device's default sampling rate and buffer size, it will utilize the \nFast Mixer\n thread.\n\n\nUse of the Android Fast Audio Path is taken care of for you in this lab, but the idea of tradeoffs between latency and buffer size is a fundamental concept in embedded audio.\n\n\n\n\nNote\n\n\nIf this is a topic that interests you, there's a \n~45 minute Google I/O talk\n that discusses the changes they've made in the last couple years to bring their latency down.\n\n\n\n\nCode Structure\n\n\nAll of your audio labs will be based on Android's \nAudio-Echo NDK sample\n. The Java side of this application should look similar to Lab 1, except instead of your \nbutton.onClickListener()\n being defined programmatically during \nonCreate()\n, it is defined in \napp/res/layout/activity_main.xml\n directly as below:\n\n\n<Button\n\n    \nandroid:id=\n\"@+id/button_start_capture\"\n\n    \nandroid:layout_width=\n\"wrap_content\"\n\n    \nandroid:layout_height=\n\"wrap_content\"\n\n    \nandroid:text=\n\"@string/StartEcho\"\n\n    \nandroid:onClick=\n\"startEcho\"\n \n/>\n\n\n\n\n\n\nNotice the field \nandroid:onClick=\"startEcho\"\n. This says that when the \nStart\n button is clicked, the function \nstartEcho()\n in \nMainActivity.java\n is called. Likewise, the same goes for \nstopEcho()\n. The most significant change between Lab 1 and Lab 2 is the introduction of the NDK. If you scroll to the bottom of \nMainActivity.java\n, you will see a host of unimplemented functions:\n\n\n/*\n\n\n* jni function implementations...\n\n\n*/\n\n\npublic\n \nstatic\n \nnative\n \nvoid\n \ncreateSLEngine\n(\nint\n \nrate\n,\n \nint\n \nframesPerBuf\n);\n\n\npublic\n \nstatic\n \nnative\n \nvoid\n \ndeleteSLEngine\n();\n\n\n\npublic\n \nstatic\n \nnative\n \nboolean\n \ncreateSLBufferQueueAudioPlayer\n();\n\n\npublic\n \nstatic\n \nnative\n \nvoid\n \ndeleteSLBufferQueueAudioPlayer\n();\n\n\n\npublic\n \nstatic\n \nnative\n \nboolean\n \ncreateAudioRecorder\n();\n\n\npublic\n \nstatic\n \nnative\n \nvoid\n \ndeleteAudioRecorder\n();\n\n\npublic\n \nstatic\n \nnative\n \nvoid\n \nstartPlay\n();\n\n\npublic\n \nstatic\n \nnative\n \nvoid\n \nstopPlay\n();\n\n\n\n\n\n\nThese functions are defined through JNI, the Java Native Interface. The JNI provides a nice way for Java to interact with functions defined in C++ using the NDK. Without going into specifics, let's take a look at what these function declarations look like in C++. Open the file \napp/cpp/audio_main.cpp\n and scroll to line 50:\n\n\nJNIEXPORT\n \nvoid\n \nJNICALL\n\n\nJava_com_ece420_lab2_MainActivity_createSLEngine\n(\nJNIEnv\n \n*\nenv\n,\n \njclass\n,\n \njint\n,\n \njint\n);\n\n\n\n\n\n\nHere we have the other side of the JNI. The function arguments contain two pointers referring back to the Java environment, then our two expected \nint\n arguments. \n\n\n\n\nNote\n\n\nYou shouldn't have to modify any of the JNI-specific code. This is provided for reference.\n\n\n\n\nYour responsibility will be to fill in the function \nece420ProcessFrame(sample_buf *dataBuf)\n in \nece420_main.cpp\n, which gets called every time the the microphone fills its \nN\n-sample buffer. This has the same effect as the Digital Filter block in the image below:\n\n\n\n\nOn completion of \nece420ProcessFrame()\n, anything inside \ndataBuf\n will be written out to the speaker (the D/A) and played. You will have to process the incoming audio data in \ndataBuf\n and overwrite the old data with your processed data to play your filtered audio. \n\n\nPart 4 - PCM-16 Decoding\n\n\nYour audio data will come from OpenSL as a buffer of 128 PCM-16 encoded samples. From \nWikipedia\n: \n\n\n\n\nPulse-code modulation (PCM) is a method used to digitally represent sampled analog signals. It is the standard form of digital audio in computers, compact discs, digital telephony and other digital audio applications. In a PCM stream, the amplitude of the analog signal is sampled regularly at uniform intervals, and each sample is quantized to the nearest value within a range of digital steps.\n\n\n\n\nYour first task in Android will be to decode this data into usable \nint16_t\n data and then back to byte-packed data. If you view the declaration for \nsample_buf\n (right click -> \nGo To\n -> \nDeclaration\n), you can see that \nsample_buf\n consists of the following: \n\n\nstruct\n \nsample_buf\n \n{\n\n    \nuint8_t\n    \n*\nbuf_\n;\n       \n// audio sample container\n\n    \nuint32_t\n    \ncap_\n;\n       \n// buffer capacity in byte\n\n    \nuint32_t\n    \nsize_\n;\n      \n// audio sample size (n buf) in byte\n\n\n};\n\n\n\n\n\n\nThe data is stored as as a \nuint8_t\n array of size \n2*N\n. OpenSL uses \nlittle endian\n byte order, meaning that the least significant byte is stored in the lower index of the array. \n\n\n\n\nAssignment\n\n\nConvert the PCM-16 data in \ndataBuf\n to \nint16_t\n and write the output to \nbufferIn\n. Also convert \nbufferOut\n to PCM-16 data and write the output back into \ndataBuf\n for the OpenSL player to read.\n\n\nYou can verify that your conversion is correct by placing a breakpoint at the end of \nece420ProcessFrame()\n and comparing the byte values in \nbuf_\n to the values in your \nint16_t\n array as below. Keep in mind that these values can be negative. \n\n\n\n\nTo view the buffer byte values in \nhex\n instead of \nchar\n, type the following in the LLDB tab in your debugging window:\n\n\n(lldb) type format add -f x uint8_t\n\n\n\n\nPart 5 - FIR Filter\n\n\nNow that you have usable data in the form of \nint16_t\n, you will implement the FIR filter you designed earlier in Python. For the sake of performance, you will be using a fixed-sized array to hold the past-sample buffer required for an FIR filter such as  \n\n\n\n\n\ny[n] = a x[n] + b x[n-1] + c x[n-2] + \\dots\n\n\n\n\n\nThe most efficient way to manage a fixed-sized array is using a circular buffer. Circular buffers maintain an index of the most recent sample and insert new data at \nmod(idx + 1, N)\n, where \nN\n is the length of your array. For a length 5 array, successive data input would look as follows:\n\n\nn = 0: {0, 1, 2, 3, 4}\nn = 1: {5, 1, 2, 3, 4}\nn = 2: {5, 6, 2, 3, 4}\nn = 3: {5, 6, 7, 3, 4}\nn = 4: {5, 6, 7, 8, 4}\nn = 5: {5, 6, 7, 8, 9}\n\n\n\n\n\n\n\nWarning\n\n\nYou may not use \nbufferIn\n directly to access prior samples for \n x[n-1], x[n-2], \n etc. This constraint will be lifted in the next lab when we begin discussing batch processing, but for now, we will avoid the boundary issues that arise when doing frame-by-frame processing. \n\n\n\n\nCode that emulates sample-by-sample processing is given in the source code and replicated below:\n\n\n// Loop code provided as a suggestion. \n\n\n// This loop simulates sample-by-sample processing.\n\n\nfor\n \n(\nint\n \nsampleIdx\n \n=\n \n0\n;\n \nsampleIdx\n \n<\n \ndataBuf\n->\nsize_\n;\n \nsampleIdx\n++\n)\n \n{\n\n    \nint16_t\n \nsample\n \n=\n \nbufferIn\n[\nsampleIdx\n];\n\n\n    \n// Your function implementation\n\n    \nint16_t\n \noutput\n \n=\n \nfirFilter\n(\nsample\n);\n\n\n    \nbufferOut\n[\nsampleIdx\n]\n \n=\n \noutput\n;\n\n\n}\n\n\n\n\n\n\n\n\n\n\n\nAssignment\n\n\nImplement the function \nint16_t firFilter(int16_t sample)\n to process data on a sample-by-sample basis. Sample history must be maintained using a circular buffer \nor\n some other Queue implementation. \n\n\nThe final result should be a real-time notch filter that listens over the microphone and attenuates frequencies around 1KHz, outputting the filtered data over the speakers. You can verify your results by playing a sine sweep into your tablet's microphone and listening to see if frequencies around 1KHz are attenuated.\n\n\n\n\nPart 6 - IIR Filter (Extra Credit)\n\n\nFor 1 point of extra credit, design and implement an IIR filter in Android that meets the same specifications as your FIR filter. \n\n\nGrading\n\n\nYour lab will be graded as follows:\n\n\n\n\nPrelab (2 points)\n\n\nLab (4 points)\n\n\nQuiz (2 points)\n\n\nExtra credit (+1 point)",
            "title": "Lab 2 - Audio Filtering"
        },
        {
            "location": "/lab2/lab/#lab-2-audio-filtering",
            "text": "",
            "title": "Lab 2 - Audio Filtering"
        },
        {
            "location": "/lab2/lab/#summary",
            "text": "In this lab, you will learn how to design FIR notch filters to specification using Python and how to implement filters efficiently in Android.",
            "title": "Summary"
        },
        {
            "location": "/lab2/lab/#downloads",
            "text": "Android Project source code",
            "title": "Downloads"
        },
        {
            "location": "/lab2/lab/#python",
            "text": "",
            "title": "Python"
        },
        {
            "location": "/lab2/lab/#part-1-filter-generation",
            "text": "Filter design in Python is very similar to filter design in MATLAB. SciPy has a  Signal Processing library  that contains built-in code for convolution, spline interpolation, filtering and filter design, as well as other signal analysis. If you are looking for a DSP function that exists in MATLAB, this library should be the first place you check.  In particular, we will be looking at  scipy.signal.firls  for our FIR filter design. Your task will be to generate the coefficients for a bandstop filter with the given specifications:   Frequencies between 1KHz and 2KHz should be attenuated to approximately -20dB or less.  All other frequencies should pass through with approximately unity gain.  Assume a sampling rate of 48kHz.   Our final goal with this filter will be to allow speech through with no attenuation but suppress pure sine tones between 1KHz and 2KHz. Some things to think about:   You can design any filter if you allow the filter order to go to infinity. What are the practical considerations to using a longer filter?  The sharper the transition bands, the larger the ripple in the passband. We've defined a relatively narrow stopband. How wide can you make the transition bands while still meeting your application's requirements?    Assignment  Generate an FIR filter using  scipy.signal.firls() . Sample code start you off and display the frequency response is given below.  Show your TA when your filter design is done. Defend your design decisions regarding points 1 and 2 from above.    Skeleton Python code shown below. Feel free to modify it as you see fit.   import   numpy   as   np  import   matplotlib.pyplot   as   plt  from   scipy   import   signal  # Your filter design here  # firls() can be called via signal.firls()  b   =   ??  # Signal analysis  w ,   h   =   signal . freqz ( b )  plt . figure ()  plt . subplot ( 2 , 1 , 1 )  plt . title ( 'Digital filter frequency response, N = '   +   str ( len ( b )))  plt . plot ( w   /   np . pi ,   20   *   np . log10 ( abs ( h )),   'b' )  plt . ylabel ( 'Amplitude [dB]' ,   color = 'b' )  plt . grid ()  plt . axis ( 'tight' )  plt . subplot ( 2 , 1 , 2 )  angles   =   np . unwrap ( np . angle ( h ))  plt . plot ( w   /   np . pi ,   angles ,   'g' )  plt . ylabel ( 'Angle (radians)' ,   color = 'g' )  plt . grid ()  plt . axis ( 'tight' )  plt . xlabel ( 'Frequency [0 to Nyquist Hz, normalized]' )  plt . show ()",
            "title": "Part 1 - Filter Generation"
        },
        {
            "location": "/lab2/lab/#part-2-fir-filter",
            "text": "Now that you have your filter coefficients, you will implement your filter in Python and test it on sample data. This prototyping stage is to get you comfortable with actually using a filter before implementing it on Android, which is significantly more difficult to debug. Your filtering code should only be a few lines long. Do not use any  scipy.signal  functions such as  lfilt  to compute the filtered output.    Assignment  Implement the filter designed in Part 1. Test your filter on the time_domain signal  test_data  given below and plot the time domain result.  Show your TA when you are done.   Python test code:  import   numpy   as   np   import   matplotlib.pyplot   as   plt   from   scipy   import   signal   F_s   =   48000  t   =   [ i   /   F_s   for   i   in   range ( 2   *   F_s )]  test_data   =   signal . chirp ( t ,   1 ,   t [ - 1 ],   24000 ,   method = 'logarithmic' )  # ... filter ...   The following code may be used to convert a Python array of coefficients to a static C++ array initialization:  a   =   [ 1 ,   2 ,   3 ,   4 ,   5 ,   6 ]  coef_str   =   \"float coefs[] = {\"  for   val   in   a : \n     coef_str   +=   str ( val )   +   \", \"  coef_str   =   coef_str [: - 2 ]  coef_str   +=   \"};\"  print ( coef_str )",
            "title": "Part 2 - FIR Filter"
        },
        {
            "location": "/lab2/lab/#android",
            "text": "",
            "title": "Android"
        },
        {
            "location": "/lab2/lab/#part-3-opensl-es",
            "text": "",
            "title": "Part 3 - OpenSL ES"
        },
        {
            "location": "/lab2/lab/#background",
            "text": "If you've worked with Android before, you may wonder why we're using the NDK for our audio transactions. An example of Java's audio recording callflow taken from  this StackOverflow post  is given below:  private   void   startRecording ()   { \n     recorder   =   new   AudioRecord ( MediaRecorder . AudioSource . MIC , \n             RECORDER_SAMPLERATE ,   RECORDER_CHANNELS , \n             RECORDER_AUDIO_ENCODING ,   BufferElements2Rec   *   BytesPerElement ); \n\n     recorder . startRecording (); \n     isRecording   =   true ; \n     recordingThread   =   new   Thread ( new   Runnable ()   { \n         public   void   run ()   { \n             writeAudioDataToFile (); \n         } \n     },   \"AudioRecorder Thread\" ); \n     recordingThread . start ();  }   In the code above, we first initialize a new  AudioRecord  instance containing all of our recording parameters, such as sampling rate, mono or stereo, encoding format, and number of elements in the audio buffer. We tell the  AudioRecord  instance to begin sampling the microphone and storing data in the buffer, then we launch a thread to monitor the buffer and extract samples when it is full (not shown).   The problem with this approach is the  latency . Android has a longstanding problem implementing  low-latency audio solutions compared to iOS . As of Q1 2015, this was reflected in the  disproportionate number of music apps available for Android versus iOS . At a high level, Android requires large sampling buffers to be filled before the data can be read. Compare this to the last lab -- it would be as if you could only read sensor data every  N  samples versus every sample. For the Java implementation given above, the minimum recording and playback buffer sizes result in about 200ms round-trip delay (RTD).  Android has made great strides in low-latency audio in the last few years, however. They have introduced a  low-latency pipeline accessible in the NDK  that operates on the  OpenSL ES framework , a standardized embedded audio API. If you use the device's default sampling rate and buffer size, it will utilize the  Fast Mixer  thread.  Use of the Android Fast Audio Path is taken care of for you in this lab, but the idea of tradeoffs between latency and buffer size is a fundamental concept in embedded audio.   Note  If this is a topic that interests you, there's a  ~45 minute Google I/O talk  that discusses the changes they've made in the last couple years to bring their latency down.",
            "title": "Background"
        },
        {
            "location": "/lab2/lab/#code-structure",
            "text": "All of your audio labs will be based on Android's  Audio-Echo NDK sample . The Java side of this application should look similar to Lab 1, except instead of your  button.onClickListener()  being defined programmatically during  onCreate() , it is defined in  app/res/layout/activity_main.xml  directly as below:  <Button \n     android:id= \"@+id/button_start_capture\" \n     android:layout_width= \"wrap_content\" \n     android:layout_height= \"wrap_content\" \n     android:text= \"@string/StartEcho\" \n     android:onClick= \"startEcho\"   />   Notice the field  android:onClick=\"startEcho\" . This says that when the  Start  button is clicked, the function  startEcho()  in  MainActivity.java  is called. Likewise, the same goes for  stopEcho() . The most significant change between Lab 1 and Lab 2 is the introduction of the NDK. If you scroll to the bottom of  MainActivity.java , you will see a host of unimplemented functions:  /*  * jni function implementations...  */  public   static   native   void   createSLEngine ( int   rate ,   int   framesPerBuf );  public   static   native   void   deleteSLEngine ();  public   static   native   boolean   createSLBufferQueueAudioPlayer ();  public   static   native   void   deleteSLBufferQueueAudioPlayer ();  public   static   native   boolean   createAudioRecorder ();  public   static   native   void   deleteAudioRecorder ();  public   static   native   void   startPlay ();  public   static   native   void   stopPlay ();   These functions are defined through JNI, the Java Native Interface. The JNI provides a nice way for Java to interact with functions defined in C++ using the NDK. Without going into specifics, let's take a look at what these function declarations look like in C++. Open the file  app/cpp/audio_main.cpp  and scroll to line 50:  JNIEXPORT   void   JNICALL  Java_com_ece420_lab2_MainActivity_createSLEngine ( JNIEnv   * env ,   jclass ,   jint ,   jint );   Here we have the other side of the JNI. The function arguments contain two pointers referring back to the Java environment, then our two expected  int  arguments.    Note  You shouldn't have to modify any of the JNI-specific code. This is provided for reference.   Your responsibility will be to fill in the function  ece420ProcessFrame(sample_buf *dataBuf)  in  ece420_main.cpp , which gets called every time the the microphone fills its  N -sample buffer. This has the same effect as the Digital Filter block in the image below:   On completion of  ece420ProcessFrame() , anything inside  dataBuf  will be written out to the speaker (the D/A) and played. You will have to process the incoming audio data in  dataBuf  and overwrite the old data with your processed data to play your filtered audio.",
            "title": "Code Structure"
        },
        {
            "location": "/lab2/lab/#part-4-pcm-16-decoding",
            "text": "Your audio data will come from OpenSL as a buffer of 128 PCM-16 encoded samples. From  Wikipedia :    Pulse-code modulation (PCM) is a method used to digitally represent sampled analog signals. It is the standard form of digital audio in computers, compact discs, digital telephony and other digital audio applications. In a PCM stream, the amplitude of the analog signal is sampled regularly at uniform intervals, and each sample is quantized to the nearest value within a range of digital steps.   Your first task in Android will be to decode this data into usable  int16_t  data and then back to byte-packed data. If you view the declaration for  sample_buf  (right click ->  Go To  ->  Declaration ), you can see that  sample_buf  consists of the following:   struct   sample_buf   { \n     uint8_t      * buf_ ;         // audio sample container \n     uint32_t      cap_ ;         // buffer capacity in byte \n     uint32_t      size_ ;        // audio sample size (n buf) in byte  };   The data is stored as as a  uint8_t  array of size  2*N . OpenSL uses  little endian  byte order, meaning that the least significant byte is stored in the lower index of the array.    Assignment  Convert the PCM-16 data in  dataBuf  to  int16_t  and write the output to  bufferIn . Also convert  bufferOut  to PCM-16 data and write the output back into  dataBuf  for the OpenSL player to read.  You can verify that your conversion is correct by placing a breakpoint at the end of  ece420ProcessFrame()  and comparing the byte values in  buf_  to the values in your  int16_t  array as below. Keep in mind that these values can be negative.    To view the buffer byte values in  hex  instead of  char , type the following in the LLDB tab in your debugging window:  (lldb) type format add -f x uint8_t",
            "title": "Part 4 - PCM-16 Decoding"
        },
        {
            "location": "/lab2/lab/#part-5-fir-filter",
            "text": "Now that you have usable data in the form of  int16_t , you will implement the FIR filter you designed earlier in Python. For the sake of performance, you will be using a fixed-sized array to hold the past-sample buffer required for an FIR filter such as     \ny[n] = a x[n] + b x[n-1] + c x[n-2] + \\dots   The most efficient way to manage a fixed-sized array is using a circular buffer. Circular buffers maintain an index of the most recent sample and insert new data at  mod(idx + 1, N) , where  N  is the length of your array. For a length 5 array, successive data input would look as follows:  n = 0: {0, 1, 2, 3, 4}\nn = 1: {5, 1, 2, 3, 4}\nn = 2: {5, 6, 2, 3, 4}\nn = 3: {5, 6, 7, 3, 4}\nn = 4: {5, 6, 7, 8, 4}\nn = 5: {5, 6, 7, 8, 9}   Warning  You may not use  bufferIn  directly to access prior samples for   x[n-1], x[n-2],   etc. This constraint will be lifted in the next lab when we begin discussing batch processing, but for now, we will avoid the boundary issues that arise when doing frame-by-frame processing.    Code that emulates sample-by-sample processing is given in the source code and replicated below:  // Loop code provided as a suggestion.   // This loop simulates sample-by-sample processing.  for   ( int   sampleIdx   =   0 ;   sampleIdx   <   dataBuf -> size_ ;   sampleIdx ++ )   { \n     int16_t   sample   =   bufferIn [ sampleIdx ]; \n\n     // Your function implementation \n     int16_t   output   =   firFilter ( sample ); \n\n     bufferOut [ sampleIdx ]   =   output ;  }     Assignment  Implement the function  int16_t firFilter(int16_t sample)  to process data on a sample-by-sample basis. Sample history must be maintained using a circular buffer  or  some other Queue implementation.   The final result should be a real-time notch filter that listens over the microphone and attenuates frequencies around 1KHz, outputting the filtered data over the speakers. You can verify your results by playing a sine sweep into your tablet's microphone and listening to see if frequencies around 1KHz are attenuated.",
            "title": "Part 5 - FIR Filter"
        },
        {
            "location": "/lab2/lab/#part-6-iir-filter-extra-credit",
            "text": "For 1 point of extra credit, design and implement an IIR filter in Android that meets the same specifications as your FIR filter.",
            "title": "Part 6 - IIR Filter (Extra Credit)"
        },
        {
            "location": "/lab2/lab/#grading",
            "text": "Your lab will be graded as follows:   Prelab (2 points)  Lab (4 points)  Quiz (2 points)  Extra credit (+1 point)",
            "title": "Grading"
        },
        {
            "location": "/lab3/lab/",
            "text": "Lab 3 - Spectrogram\n\n\nSummary\n\n\nIn this lab, you will investigate the effects of windowing, zero-padding, and batch-processing in the frequency domain by way of the Short-Time Fourier Transform (STFT).\n\n\n\n\nVideo of the final app as-is\n\n\nVideo of the final app with extra credit implemented\n\n\n\n\nDownloads\n\n\n\n\nPython test harness and test vectors\n\n\nAndroid project source code\n\n\n\n\nPython\n\n\n\n\n\nPhilosophical Overview\n\n\nFrom this lab onward, Python will primarily be used as a tool to prototype your full system before beginning implementation on Android. It may be tempting to dive into the final implementation immediately, but keep in mind the benefits of a higher-level language:\n\n\n\n\nUsing offline test vectors is significantly easier.\n\n\nCommon functions are often built-in.\n\n\nTesting quick changes does not require recompiling and loading onto the tablet. \n\n\nPrototyping your implementation does not require excessive scaffolding code.\n\n\n\n\nAll told, \niterating in Python is significantly quicker than in Android\n.\n\n\nSystem Description\n\n\nIn this lab, you will implement an online spectrogram comprising the block diagram below. The ability to overlap, while enormously important to constructing a spectrogram that is smooth in time, will require modifying additional Android code and thus is left optional.\n\n\n\n\nYour input signal will be some .wav file of significant enough length to motivate the use of a STFT rather than a single Fourier transform over the entire .wav file. The provided Python test code will simulate the Android buffer architecture by breaking the sample data into blocks determined by the bandwidth of the signal.\n\n\n\n\nQuestion\n\n\nWhy would you use a STFT over a single-snapshot FFT?\n\n\n\n\nYou will first window your input buffer using a \nHamming window\n defined by the following function:\n\n\n\n\n\nw[n] = 0.54 - 0.46 \\cos \\left( \\frac{2 \\pi n}{N - 1} \\right), \\quad 0 \\le n \\le N - 1\n\n\n\n\n\n\n\nN\n is defined in the provided code as \nFRAME_SIZE\n. You will then zero-pad by a factor of two and compute the magnitude of the FFT. Keep in mind that the FFT has both a real and an imaginary component. You should only keep the first half of your FFT output. \n\n\n\n\nQuestion\n\n\nWhat allows us to ignore the second half of our FFT output? Recall the Conjugate Symmetry property of the Fourier Transform.\n\n\n\n\nFinally, you will scale your magnitude output logarithmically. Consider Prelab 2, when the original signal was corrupted with a pure sine tone at 400Hz. The unscaled FFT's dynamic range is large enough that a single dominant frequency bin can overpower the remaining frequency bins when normalizing from 0 to 1. Your STFT output will be mapped to a heatmap scaling between 0 and 1, so you may need to do additional scaling after scaling logarithmically.\n\n\n\n\nAssignment\n\n\nImplement the system described in the block diagram above for the test code given. For every block of data, your output should be the scaled power spectrum \nP(s) = |F(s)|^2\n such that values are (primarily) between 0 and 1. This will be evident in the constructed spectrogram. Label the axes of the spectrogram so that X and Y correspond to milliseconds and Hz, respectively.\n\n\nSend this Python script to your TA in the format \nlab3_firstname_lastname.py\n on completion. Grading will be based on the following:\n\n\n\n\nDid you implement each of the required blocks? \n\n\nDoes your scaling produce reasonable output?\n\n\n\n\nSome examples of reasonable-looking spectrograms can be found \nhere\n, \nhere\n, and \nhere\n. Extra credit will not be awarded for implementing buffer overlapping in Python, however it is recommended to prototype buffer overlapping in Python to earn the extra credit points for Android.\n\n\n\n\nPython Hints\n\n\n\n\nMultiplying two NumPy vectors of the same length \nc = a * b\n results in an element-wise multiplication such that \nc[0] = a[0] * b[0]; c[1] = a[1] * b[1]\n, etc.\n\n\nPython arrays (and NumPy arrays/vectors) can be sliced to only access certain portions. For example, to only write to the second half of an array, you can do the following: \na[len(a)/2:] = b\n. \n\n\nThe size of a NumPy array (similar to MATLAB's \nsize()\n function) is given by \nyourArray.shape\n.\n\n\n\n\nTest code\n\n\nFor the next three labs (including this one), we will be giving you Python test code which simulates the Android callflow on some offline test vector. If you are careful not to rely on Python-specific function calls such as \nnp.convolve()\n or \nnp.lfilter()\n, converting your code to Android should be a smooth experience.\n\n\nExample code\n\n\n# Scaffolding code omitted\n\n\n\nFRAME_SIZE\n \n=\n \n1024\n\n\nZP_FACTOR\n \n=\n \n2\n\n\nFFT_SIZE\n \n=\n \nFRAME_SIZE\n \n*\n \nZP_FACTOR\n\n\n\n################## YOUR CODE HERE ######################\n\n\ndef\n \nece420ProcessFrame\n(\nframe\n):\n\n    \ncurFft\n \n=\n \nnp\n.\nzeros\n(\nFFT_SIZE\n)\n\n\n    \nreturn\n \ncurFft\n\n\n\n\n\n\nAndroid\n\n\nSystem Overview\n\n\nAs in Lab 2, you will receive a buffer from the microphone via \nece420ProcessFrame(sample_buf *dataBuf)\n and process your data within the timing window allotted by OpenSL. We have given you the code to convert from byte-packed PCM-16 to \nint16_t\n, and we have made the additional decision to work with \nfloat\n instead of \nint16_t\n. \n\n\n\n\nFloating Point versus Fixed Point\n\n\nFixed point used to be the de facto standard in embedded DSP computing due to readily-available hardware acceleration for fixed point multiplications and additions. Floating point computation was traditionally done in software, meaning that computations would involve multiple CPU cycles rather than a single assembly call. \n\n\nAny new Android device using the ARMv7 architecture now contains ARM VFP, a floating point coprocessor which handles half, single, and double-precision floating point arithmetic in hardware. The difference between computation time is now negligible and only necessary as a final optimization step if your timing budget is microseconds short.\n\n\nYou can test this claim inside \nece420ProcessFrame()\n. Comment all the code except for the timing code wrapper, and try computing a dummy computation of one hundred thousand float multiply/adds versus \nint16_t\n multiply/adds. \n\n\n\n\nWe will be using the \nKiss FFT\n library for computing the FFT in Android. The engineering philosophy of KISS stands for \n\"Keep it simple, stupid\"\n, and as such, the library only consists of a single source and header file. Importing the library has already been done for you, but you will have to figure out how to use it. There is a helpful README packaged with the library under \napp/src/main/cpp/kiss_fft\n.\n\n\nSomething to keep in mind when handling audio data is how long it takes to process your data. Timing is not a problem when dealing with offline test vectors, but on Android, your code is under the power of the operating system. If you take too long while processing a buffer, you may miss the next buffer and cause output audio artifacts or even system crashes.\n\n\n\n\nExercise\n\n\nGiven a buffer size \nN\n and a sampling rate \nF_s\n, how much time do you have to complete your processing before the next buffer comes in? \n\n\n\n\n\n\nAssignment\n\n\nImplement the system described in the block diagram earlier by filling in the function \nece420ProcessFrame()\n inside \ncpp/ece420_main.cpp\n. Your code should closely match your Python implementation. You should write your output to the global variable \nfftOut\n so it can be read by the UI-updating thread. \n\n\nYour demo will consist of speech input and single-tone input at various volumes. You should verify your platform beforehand on a variety of sounds at different volumes. \n\n\n\n\nBuffer Overlap (Extra Credit)\n\n\nFor 1 point of extra credit, implement the buffer overlap portion of the block diagram above in Android. This means computing the FFT over a sliding window of samples instead of mutually-exclusive buffers. You will have to change the OpenSL buffer size in \nMainActivity.java\n, along with a few other things, but the total additional code should be no more than 20 lines.\n\n\nGrading\n\n\nYour lab will be graded as follows:\n\n\n\n\nPrelab (2 points) \n\n\nLab (4 points) \n\n\nQuiz (2 points) \n\n\nExtra Credit (+1 point)",
            "title": "Lab 3 - Spectrogram"
        },
        {
            "location": "/lab3/lab/#lab-3-spectrogram",
            "text": "",
            "title": "Lab 3 - Spectrogram"
        },
        {
            "location": "/lab3/lab/#summary",
            "text": "In this lab, you will investigate the effects of windowing, zero-padding, and batch-processing in the frequency domain by way of the Short-Time Fourier Transform (STFT).   Video of the final app as-is  Video of the final app with extra credit implemented",
            "title": "Summary"
        },
        {
            "location": "/lab3/lab/#downloads",
            "text": "Python test harness and test vectors  Android project source code",
            "title": "Downloads"
        },
        {
            "location": "/lab3/lab/#python",
            "text": "",
            "title": "Python"
        },
        {
            "location": "/lab3/lab/#philosophical-overview",
            "text": "From this lab onward, Python will primarily be used as a tool to prototype your full system before beginning implementation on Android. It may be tempting to dive into the final implementation immediately, but keep in mind the benefits of a higher-level language:   Using offline test vectors is significantly easier.  Common functions are often built-in.  Testing quick changes does not require recompiling and loading onto the tablet.   Prototyping your implementation does not require excessive scaffolding code.   All told,  iterating in Python is significantly quicker than in Android .",
            "title": "Philosophical Overview"
        },
        {
            "location": "/lab3/lab/#system-description",
            "text": "In this lab, you will implement an online spectrogram comprising the block diagram below. The ability to overlap, while enormously important to constructing a spectrogram that is smooth in time, will require modifying additional Android code and thus is left optional.   Your input signal will be some .wav file of significant enough length to motivate the use of a STFT rather than a single Fourier transform over the entire .wav file. The provided Python test code will simulate the Android buffer architecture by breaking the sample data into blocks determined by the bandwidth of the signal.   Question  Why would you use a STFT over a single-snapshot FFT?   You will first window your input buffer using a  Hamming window  defined by the following function:   \nw[n] = 0.54 - 0.46 \\cos \\left( \\frac{2 \\pi n}{N - 1} \\right), \\quad 0 \\le n \\le N - 1    N  is defined in the provided code as  FRAME_SIZE . You will then zero-pad by a factor of two and compute the magnitude of the FFT. Keep in mind that the FFT has both a real and an imaginary component. You should only keep the first half of your FFT output.    Question  What allows us to ignore the second half of our FFT output? Recall the Conjugate Symmetry property of the Fourier Transform.   Finally, you will scale your magnitude output logarithmically. Consider Prelab 2, when the original signal was corrupted with a pure sine tone at 400Hz. The unscaled FFT's dynamic range is large enough that a single dominant frequency bin can overpower the remaining frequency bins when normalizing from 0 to 1. Your STFT output will be mapped to a heatmap scaling between 0 and 1, so you may need to do additional scaling after scaling logarithmically.   Assignment  Implement the system described in the block diagram above for the test code given. For every block of data, your output should be the scaled power spectrum  P(s) = |F(s)|^2  such that values are (primarily) between 0 and 1. This will be evident in the constructed spectrogram. Label the axes of the spectrogram so that X and Y correspond to milliseconds and Hz, respectively.  Send this Python script to your TA in the format  lab3_firstname_lastname.py  on completion. Grading will be based on the following:   Did you implement each of the required blocks?   Does your scaling produce reasonable output?   Some examples of reasonable-looking spectrograms can be found  here ,  here , and  here . Extra credit will not be awarded for implementing buffer overlapping in Python, however it is recommended to prototype buffer overlapping in Python to earn the extra credit points for Android.",
            "title": "System Description"
        },
        {
            "location": "/lab3/lab/#python-hints",
            "text": "Multiplying two NumPy vectors of the same length  c = a * b  results in an element-wise multiplication such that  c[0] = a[0] * b[0]; c[1] = a[1] * b[1] , etc.  Python arrays (and NumPy arrays/vectors) can be sliced to only access certain portions. For example, to only write to the second half of an array, you can do the following:  a[len(a)/2:] = b .   The size of a NumPy array (similar to MATLAB's  size()  function) is given by  yourArray.shape .",
            "title": "Python Hints"
        },
        {
            "location": "/lab3/lab/#test-code",
            "text": "For the next three labs (including this one), we will be giving you Python test code which simulates the Android callflow on some offline test vector. If you are careful not to rely on Python-specific function calls such as  np.convolve()  or  np.lfilter() , converting your code to Android should be a smooth experience.",
            "title": "Test code"
        },
        {
            "location": "/lab3/lab/#example-code",
            "text": "# Scaffolding code omitted  FRAME_SIZE   =   1024  ZP_FACTOR   =   2  FFT_SIZE   =   FRAME_SIZE   *   ZP_FACTOR  ################## YOUR CODE HERE ######################  def   ece420ProcessFrame ( frame ): \n     curFft   =   np . zeros ( FFT_SIZE ) \n\n     return   curFft",
            "title": "Example code"
        },
        {
            "location": "/lab3/lab/#android",
            "text": "",
            "title": "Android"
        },
        {
            "location": "/lab3/lab/#system-overview",
            "text": "As in Lab 2, you will receive a buffer from the microphone via  ece420ProcessFrame(sample_buf *dataBuf)  and process your data within the timing window allotted by OpenSL. We have given you the code to convert from byte-packed PCM-16 to  int16_t , and we have made the additional decision to work with  float  instead of  int16_t .    Floating Point versus Fixed Point  Fixed point used to be the de facto standard in embedded DSP computing due to readily-available hardware acceleration for fixed point multiplications and additions. Floating point computation was traditionally done in software, meaning that computations would involve multiple CPU cycles rather than a single assembly call.   Any new Android device using the ARMv7 architecture now contains ARM VFP, a floating point coprocessor which handles half, single, and double-precision floating point arithmetic in hardware. The difference between computation time is now negligible and only necessary as a final optimization step if your timing budget is microseconds short.  You can test this claim inside  ece420ProcessFrame() . Comment all the code except for the timing code wrapper, and try computing a dummy computation of one hundred thousand float multiply/adds versus  int16_t  multiply/adds.    We will be using the  Kiss FFT  library for computing the FFT in Android. The engineering philosophy of KISS stands for  \"Keep it simple, stupid\" , and as such, the library only consists of a single source and header file. Importing the library has already been done for you, but you will have to figure out how to use it. There is a helpful README packaged with the library under  app/src/main/cpp/kiss_fft .  Something to keep in mind when handling audio data is how long it takes to process your data. Timing is not a problem when dealing with offline test vectors, but on Android, your code is under the power of the operating system. If you take too long while processing a buffer, you may miss the next buffer and cause output audio artifacts or even system crashes.   Exercise  Given a buffer size  N  and a sampling rate  F_s , how much time do you have to complete your processing before the next buffer comes in?     Assignment  Implement the system described in the block diagram earlier by filling in the function  ece420ProcessFrame()  inside  cpp/ece420_main.cpp . Your code should closely match your Python implementation. You should write your output to the global variable  fftOut  so it can be read by the UI-updating thread.   Your demo will consist of speech input and single-tone input at various volumes. You should verify your platform beforehand on a variety of sounds at different volumes.",
            "title": "System Overview"
        },
        {
            "location": "/lab3/lab/#buffer-overlap-extra-credit",
            "text": "For 1 point of extra credit, implement the buffer overlap portion of the block diagram above in Android. This means computing the FFT over a sliding window of samples instead of mutually-exclusive buffers. You will have to change the OpenSL buffer size in  MainActivity.java , along with a few other things, but the total additional code should be no more than 20 lines.",
            "title": "Buffer Overlap (Extra Credit)"
        },
        {
            "location": "/lab3/lab/#grading",
            "text": "Your lab will be graded as follows:   Prelab (2 points)   Lab (4 points)   Quiz (2 points)   Extra Credit (+1 point)",
            "title": "Grading"
        },
        {
            "location": "/lab4/lab/",
            "text": "Lab 4 - Pitch Detection\n\n\nSummary\n\n\nIn this lab, you will learn how to detect the pitch of a signal in real time via autocorrelation. An example of the final solution can be found \nhere\n. Next lab will utilize this pitch detector in order to do pitch synthesis a la Auto-Tune. \n\n\nDownloads\n\n\n\n\nPython test harness and test vector\n\n\nAndroid project source code\n\n\n\n\nPython\n\n\nAs in Lab 3, your task for the Python portion of this lab will be to prototype your Android system. For every frame of audio data, you will determine whether the frame contains background noise or a voiced signal. You will then compute the autocorrelation using a fast frequency-based method. Finally, you will determine the pitch from the autocorrelation output. These blocks are described in further detail below. \n\n\n\n\n\n\nNote\n\n\nFor the sake of completeness, the notes from the prelab describing voiced/unvoiced detection and autocorrelation are reproduced below.\n\n\n\n\n\n\n\nVoiced/Unvoiced Detector\n\n\nVoiced/unvoiced signal classification is an \nincredibly well-studied field\n with a number of vetted solutions such as \nRabiner's pattern recognition approach\n or \nBachu's zero-crossing rate approach\n. Pitch shifting (next lab) does not require highly-accurate voiced/unvoiced detection however, so we will use a much simpler technique.\n\n\nThe energy of a signal can be a useful surrogate for voiced/unvoiced classification. Put simply, if a signal has enough energy, we assume it is voiced and continue our pitch analysis. \nThe energy of a discrete-time signal is given as follows:\n\n\n\n\n\n    E_s = \\sum_{n=-\\infty}^{\\infty} |x(n)|^2\n\n\n\n\n\nIn this block, you will have to determine a useful threshold for \nE_s\n and classify frames as voiced or unvoiced depending on \nE_s\n.\n\n\nAutocorrelation\n\n\nAutocorrelation is the process of circularly convolving a signal with itself. That is, for a real signal, the discrete autocorrelation is given as:\n\n\n\n\n\n    R_{xx}[l] = x[n] \\circledast \\tilde{x}[-n],\n\n\n\n\n\nwhere \n\\tilde{x}[-n]\n is the complex conjugate of the time reversal of \nx[n]\n. The output \nR_{xx}[l]\n measures how self-similar a signal is if shifted by some lag \nl\n. If normalized to 1 at zero lag, this can be written equivalently as:\n\n\n\n\n\n  R_{xx}[l] = \\frac{\\sum_{n=0}^{N-1} x[n] x[n - l]}{\\sum_{n=0}^{N-1} x[n]^2}\n\n\n\n\n\nFor a periodic signal, the lag \nl\n that maximizes \nR_{xx}[l]\n indicates the frequency of the signal. In other words, the signal takes \nl\n samples before repeating itself. This algorithm, combined with some additional modifications to prevent harmonics from being detected, comprises the \nmost well-known frequency estimator for speech and music\n.\n\n\nBig O Notation\n\n\nUnfortunately, computing \nR_{xx}\n in the time domain is an extremely slow operation. The complexity in \nBig O Notation\n is \nO(N^2)\n, meaning that if a signal has \nN\n samples, computing the autocorrelation will require \nN^2\n operations. The real-world ramifications of this is that computing the autocorrelation will be too slow to fit in our timing window.\n\n\nFortunately, there is a way around this. Recall that convolution in time is the same as multiplication in frequency. Convolution is generally an \nO(N^2)\n operation, but computing the FFT of a signal is only \nO(N \\log N)\n. Converting to the frequency domain then, we have the following equation:\n\n\n\n\n\n    R_{xx}[l] = \\mathcal{F}^{-1}\\left\\{ \\mathcal{F}\\left\\{x\\right\\} \\tilde{\\mathcal{F}}\\left\\{x\\right\\} \\right\\},\n\n\n\n\n\nor in pseudocode, \nautoc = ifft(fft(x) * conj(fft(x))\n. This is an enormously powerful result. Computing the FFT or inverse FFT is only \nO(N \\log N)\n, and element-wise multiplication only \nO(N)\n. We can reuse the result of one of the FFTs to only require one FFT, one inverse FFT, and one full multiplication. Our total computational complexity is then \nO(2 N \\log N + N)\n. Big O notation typically drops all but the dominant term, so our final cost is considered \nO(N \\log N)\n.\n\n\n\n\nNote\n\n\nFor visualization, the table below demonstrates the growth in \nN\n of the two Big O costs.\n\n\n\n\n\n\n\n\n\n\nN\n\n\n\n\n\n\nO(N^2)\n number of operations\n\n\n\n\nO(N \\log N)\n number of operations\n\n\nRatio of \nO(N \\log N)\n to \nO(N^2)\n\n\n\n\n\n\n\n\n\n\n\n\n10\n\n\n100\n\n\n10\n\n\n0.1\n\n\n\n\n\n\n100\n\n\n10000\n\n\n200\n\n\n0.02\n\n\n\n\n\n\n1000\n\n\n1000000\n\n\n3000\n\n\n0.003\n\n\n\n\n\n\n10000\n\n\n100000000\n\n\n40000\n\n\n0.0004\n\n\n\n\n\n\n100000\n\n\n10000000000\n\n\n500000\n\n\n0.00005\n\n\n\n\n\n\n1000000\n\n\n1000000000000\n\n\n6000000\n\n\n0.000006\n\n\n\n\n\n\n\n\nFor our frame length of \nN = 2048 \n, we get a speedup of around 600x (approximately) using the frequency domain method.\n\n\n\n\nSystem Requirements\n\n\nThe Python test code has been given to mimic the final Android system. You will get an input frame of size 2048, corresponding to about 40 ms of data, and you will determine the pitch (if it is voiced). If the pitch is unvoiced, you should return -1. Some things to think about:\n\n\n\n\nThe autocorrelation for speech signals will be periodic with many candidate peaks. How do you decide which peak to use? \n\n\nThe autocorrelation for any signal will be maximal in the neighborhood surrounding zero lag. How do you decide what to ignore? \n\n\nWhy did we choose 40 ms frames?\n\n\n\n\n\n\nAssignment\n\n\nImplement the Python system described above, and plot the output frequency over time. Also compute the output frequency for \nN\n equals 512, 1024, 2048, 4096, and 8192. What is the resolution and detectable frequency range (minimum and maximum pitch) for a given \nN\n? These plots and discussion on resolution/range will count for 2 demo points.\n\n\nDo not rely on built-in functions such as \nnp.correlate()\n, and don't worry about frame boundaries.\n\n\n\n\nAndroid\n\n\nIf you've implemented your Python system without relying on many built-ins, it should translate nicely to Android. Your code will reside in \ncpp/ece420_main.cpp:ece420ProcessFrame()\n as before. The library \nece420_lib.cpp\n contains some functions that you may find useful. Namely, \nfindMaxArrayIdx()\n will return the index of the maximum value in a \nfloat\n array, given some minimum index (inclusive) and some maximum index (exclusive). For example:\n\n\nfloat\n \narr\n[\n10\n]\n \n=\n \n{\n8\n,\n \n2\n,\n \n3\n,\n \n4\n,\n \n5\n,\n \n4\n,\n \n3\n,\n \n2\n,\n \n1\n,\n \n10\n};\n\n\n\n// Finds the index of the maximum value only considering indices 3, 4, 5, 6\n\n\n// Remember, C++ and Python are both zero-indexed\n\n\nint\n \nmaxIdx\n \n=\n \nfindMaxArrayIdx\n(\narr\n,\n \n3\n,\n \n7\n);\n\n\n\n// maxIdx == 5\n\n\n\n\n\n\n\n\nAssignment\n\n\nImplement the block diagram described in the Python section in \ncpp/ece420_main.cpp:ece420ProcessFrame()\n. At the end of each processing block, write your detected frequency to \nlastFreqDetected\n. If the buffer was unvoiced, write \n-1\n to \nlastFreqDetected\n. \n\n\nThe Android implementation will count for two demo points. The following will be considered when grading:\n\n\n\n\nIs the voiced/unvoiced detection reasonable?\n\n\nAre you accurately detecting the fundamental frequency and not the harmonics? \n\n\n\n\n\n\nGrading\n\n\nYour lab will be graded as follows:\n\n\n\n\nPrelab (2 points) \n\n\nLab (4 points) \n\n\nQuiz (2 points)",
            "title": "Lab 4 - Pitch Detection"
        },
        {
            "location": "/lab4/lab/#lab-4-pitch-detection",
            "text": "",
            "title": "Lab 4 - Pitch Detection"
        },
        {
            "location": "/lab4/lab/#summary",
            "text": "In this lab, you will learn how to detect the pitch of a signal in real time via autocorrelation. An example of the final solution can be found  here . Next lab will utilize this pitch detector in order to do pitch synthesis a la Auto-Tune.",
            "title": "Summary"
        },
        {
            "location": "/lab4/lab/#downloads",
            "text": "Python test harness and test vector  Android project source code",
            "title": "Downloads"
        },
        {
            "location": "/lab4/lab/#python",
            "text": "As in Lab 3, your task for the Python portion of this lab will be to prototype your Android system. For every frame of audio data, you will determine whether the frame contains background noise or a voiced signal. You will then compute the autocorrelation using a fast frequency-based method. Finally, you will determine the pitch from the autocorrelation output. These blocks are described in further detail below.     Note  For the sake of completeness, the notes from the prelab describing voiced/unvoiced detection and autocorrelation are reproduced below.",
            "title": "Python"
        },
        {
            "location": "/lab4/lab/#voicedunvoiced-detector",
            "text": "Voiced/unvoiced signal classification is an  incredibly well-studied field  with a number of vetted solutions such as  Rabiner's pattern recognition approach  or  Bachu's zero-crossing rate approach . Pitch shifting (next lab) does not require highly-accurate voiced/unvoiced detection however, so we will use a much simpler technique.  The energy of a signal can be a useful surrogate for voiced/unvoiced classification. Put simply, if a signal has enough energy, we assume it is voiced and continue our pitch analysis.  The energy of a discrete-time signal is given as follows:   \n    E_s = \\sum_{n=-\\infty}^{\\infty} |x(n)|^2   In this block, you will have to determine a useful threshold for  E_s  and classify frames as voiced or unvoiced depending on  E_s .",
            "title": "Voiced/Unvoiced Detector"
        },
        {
            "location": "/lab4/lab/#autocorrelation",
            "text": "Autocorrelation is the process of circularly convolving a signal with itself. That is, for a real signal, the discrete autocorrelation is given as:   \n    R_{xx}[l] = x[n] \\circledast \\tilde{x}[-n],   where  \\tilde{x}[-n]  is the complex conjugate of the time reversal of  x[n] . The output  R_{xx}[l]  measures how self-similar a signal is if shifted by some lag  l . If normalized to 1 at zero lag, this can be written equivalently as:   \n  R_{xx}[l] = \\frac{\\sum_{n=0}^{N-1} x[n] x[n - l]}{\\sum_{n=0}^{N-1} x[n]^2}   For a periodic signal, the lag  l  that maximizes  R_{xx}[l]  indicates the frequency of the signal. In other words, the signal takes  l  samples before repeating itself. This algorithm, combined with some additional modifications to prevent harmonics from being detected, comprises the  most well-known frequency estimator for speech and music .",
            "title": "Autocorrelation"
        },
        {
            "location": "/lab4/lab/#big-o-notation",
            "text": "Unfortunately, computing  R_{xx}  in the time domain is an extremely slow operation. The complexity in  Big O Notation  is  O(N^2) , meaning that if a signal has  N  samples, computing the autocorrelation will require  N^2  operations. The real-world ramifications of this is that computing the autocorrelation will be too slow to fit in our timing window.  Fortunately, there is a way around this. Recall that convolution in time is the same as multiplication in frequency. Convolution is generally an  O(N^2)  operation, but computing the FFT of a signal is only  O(N \\log N) . Converting to the frequency domain then, we have the following equation:   \n    R_{xx}[l] = \\mathcal{F}^{-1}\\left\\{ \\mathcal{F}\\left\\{x\\right\\} \\tilde{\\mathcal{F}}\\left\\{x\\right\\} \\right\\},   or in pseudocode,  autoc = ifft(fft(x) * conj(fft(x)) . This is an enormously powerful result. Computing the FFT or inverse FFT is only  O(N \\log N) , and element-wise multiplication only  O(N) . We can reuse the result of one of the FFTs to only require one FFT, one inverse FFT, and one full multiplication. Our total computational complexity is then  O(2 N \\log N + N) . Big O notation typically drops all but the dominant term, so our final cost is considered  O(N \\log N) .   Note  For visualization, the table below demonstrates the growth in  N  of the two Big O costs.      N    O(N^2)  number of operations   O(N \\log N)  number of operations  Ratio of  O(N \\log N)  to  O(N^2)       10  100  10  0.1    100  10000  200  0.02    1000  1000000  3000  0.003    10000  100000000  40000  0.0004    100000  10000000000  500000  0.00005    1000000  1000000000000  6000000  0.000006     For our frame length of  N = 2048  , we get a speedup of around 600x (approximately) using the frequency domain method.",
            "title": "Big O Notation"
        },
        {
            "location": "/lab4/lab/#system-requirements",
            "text": "The Python test code has been given to mimic the final Android system. You will get an input frame of size 2048, corresponding to about 40 ms of data, and you will determine the pitch (if it is voiced). If the pitch is unvoiced, you should return -1. Some things to think about:   The autocorrelation for speech signals will be periodic with many candidate peaks. How do you decide which peak to use?   The autocorrelation for any signal will be maximal in the neighborhood surrounding zero lag. How do you decide what to ignore?   Why did we choose 40 ms frames?    Assignment  Implement the Python system described above, and plot the output frequency over time. Also compute the output frequency for  N  equals 512, 1024, 2048, 4096, and 8192. What is the resolution and detectable frequency range (minimum and maximum pitch) for a given  N ? These plots and discussion on resolution/range will count for 2 demo points.  Do not rely on built-in functions such as  np.correlate() , and don't worry about frame boundaries.",
            "title": "System Requirements"
        },
        {
            "location": "/lab4/lab/#android",
            "text": "If you've implemented your Python system without relying on many built-ins, it should translate nicely to Android. Your code will reside in  cpp/ece420_main.cpp:ece420ProcessFrame()  as before. The library  ece420_lib.cpp  contains some functions that you may find useful. Namely,  findMaxArrayIdx()  will return the index of the maximum value in a  float  array, given some minimum index (inclusive) and some maximum index (exclusive). For example:  float   arr [ 10 ]   =   { 8 ,   2 ,   3 ,   4 ,   5 ,   4 ,   3 ,   2 ,   1 ,   10 };  // Finds the index of the maximum value only considering indices 3, 4, 5, 6  // Remember, C++ and Python are both zero-indexed  int   maxIdx   =   findMaxArrayIdx ( arr ,   3 ,   7 );  // maxIdx == 5    Assignment  Implement the block diagram described in the Python section in  cpp/ece420_main.cpp:ece420ProcessFrame() . At the end of each processing block, write your detected frequency to  lastFreqDetected . If the buffer was unvoiced, write  -1  to  lastFreqDetected .   The Android implementation will count for two demo points. The following will be considered when grading:   Is the voiced/unvoiced detection reasonable?  Are you accurately detecting the fundamental frequency and not the harmonics?",
            "title": "Android"
        },
        {
            "location": "/lab4/lab/#grading",
            "text": "Your lab will be graded as follows:   Prelab (2 points)   Lab (4 points)   Quiz (2 points)",
            "title": "Grading"
        },
        {
            "location": "/lab5/lab/",
            "text": "Lab 5 - Pitch Synthesis\n\n\nSummary\n\n\nIn this lab, you will learn about a simplified model for human speech and how to exploit this model to shift the frequency of an incoming signal. You will also learn one method for producing continuous output within a batch processing framework.\n\n\nDownloads\n\n\n\n\nPython test harness and test vector TBA\n\n\nAndroid project source code TBA\n\n\n\n\n\n\nSome Remarks\n\n\nWe have completed the scaffolding of the TD-PSOLA algorithm for you, leaving remapping, windowing, and overlap-add for your lab assignment. You will be responsible for knowing why our buffering scheme guarantees smoothness in the signal reconstruction, and you will also be responsible for knowing why the characteristics of human speech permit such an algorithm.\n\n\nWe will try to explain these in detail in the lab notes, but please do not hesitate to ask your TA or ask on Piazza if these details are unclear. \n\n\n\n\nTD-PSOLA\n\n\nTime-Domain Pitch Synchronous Overlap-Add, or TD-PSOLA, is the pitch-shifting algorithm we will be using in this lab. It relies heavily the source-filter model of speech, detailed below.\n\n\nSource-Filter Model of Speech\n\n\nWe can model speech as the output of two successive blocks, a source block and filter block:\n\n\n\n\nOur vocal cords are the source, and we model the output of our vocal cords as a delta train (also called a Dirac comb). Of course, our vocal cords cannot truly produce zero-width impulses of energy, but this assumption is sufficient for modeling purposes. This series of impulses determines the fundamental frequency, or the pitch, of our utterance. \n\n\nThe impulses of energy produced by our vocal cords then travel up through our vocal tract. Everything in your vocal tract (lips, tongue throat, mouth cavity, nasal cavity, among others) affect the final waveform. We make the assumption that these effects are linear and can be combined without penalty, so we call everything inside the vocal tract the \nfilter\n.\n\n\nSimplifying the complex problem of speech generation in this manner allows us to apply some signal processing tools we learned in ECE 310. In particular, if we assume that speech is nothing but the output of a filter, we can rewrite speech in the form of a transfer function where \ny(t)\n is the utterance, \nx(t)\n is the source, and \nh(t)\n is the filter:\n\n\n\n\n\ny(t) = x(t) * h(t)\n\n\n\n\n\nNote that \n*\n denotes the convolution operator. We can further rewrite \nx(t)\n, assuming \nx(t)\n is a delta train with some period \nP\n:\n\n\n\n\n\n\n\n\n\\begin{align}\n    y(t) &= \\left(\\sum_{k=0}^\\infty \\delta(t - k P)\\right) * h(t) \\\\\n         &= \\sum_{k=0}^\\infty \\delta(t - k P) * h(t) \\\\\n         &= \\sum_{k=0}^\\infty h(t - k P)\n\\end{align}    \n\n\n\n\n\nThis is a powerful result which says that the fundamental frequency (the pitch) does not depend on the filter \nh(t)\n, but on the source \nx(t)\n. In other words, speech can be decomposed into a sum of impulse responses, where the spacing of the impulse responses \nP\n determines the pitch. \n\n\nIf we want to shift the pitch, all we need is some way of identifying the impulse response from the speech signal. We can then extract the impulse response and place it with any spacing \nP\n in the synthesis signal we desire.\n\n\n\n\n\nTD-PSOLA Explained\n\n\nConsidering the discussion above, we can rewrite our problem more formally. Given a signal \ny_{P_0}(t)\n with some fundamental period \nP_0\n:\n\n\n\n\n\ny_{P_0}(t) = \\sum_{k=0}^\\infty h(t - k P_0),\n\n\n\n\n\ncompute a new synthesis signal \ny_{P_1}(t)\n with a new period \nP_1\n:\n\n\n\n\n\n\\begin{align}\n    y_{P_1}(t) &= \\sum_{k=0}^\\infty h(t - k P_1).\n\\end{align}\n\n\n\n\n\nIn order to compute the new synthesis signal, we need some way of extracting the impulse response from \ny_{P_0}(t)\n. This is done by identifying the period (using the technique from Lab 4) and further identifying each \nepoch\n in the signal, or the location of each pitch period, by finding peaks in the signal approximately \nP_0\n samples apart. An example of this for a sample frame is shown below.\n\n\n\n\nWe can then extract the impulse response (rather, an estimate of the impulse response) by windowing \n\\pm P_0\n about each epoch marker. For example, when windowed with a Hanning window, an individual response may look like this:\n\n\n\n\nNow that we have a method of estimating the impulse responses, we simply need to map from the original spacing \ny_{P_0}(t) = \\sum_{k=0}^\\infty h(t - k P_0)\n to the synthesis spacing \ny_{P_1}(t) = \\sum_{k=0}^\\infty h(t - k P_1)\n. In other words, we need to determine where the original impulse responses should go in our new signal.\n\n\nImagine our original signal has epochs spaced as below:\n\n\norig\n:\n \n1\n     \n2\n     \n3\n     \n4\n     \n5\n     \n6\n     \n7\n     \n8\n\n\n\n\n\n\nand we want to double the frequency of the original signal without changing the duration. This can be achieved by spacing our new epochs at \nP_1 = \\frac{P_0}{2}\n. For every new epoch at \n\\left\\{ P_1, 2 P_1, 3 P_1, \\dots \\right\\}\n, we need to find the nearest epoch in the original signal. This mapping is shown below for our example signal:\n\n\norig\n:\n \n1\n     \n2\n     \n3\n     \n4\n     \n5\n     \n6\n     \n7\n     \n8\n\n      \n|\\\n    \n|\\\n    \n|\\\n    \n|\\\n    \n|\\\n    \n|\\\n    \n|\\\n    \n|\n\n      \n|\n \n\\\n   \n|\n \n\\\n   \n|\n \n\\\n   \n|\n \n\\\n   \n|\n \n\\\n   \n|\n \n\\\n   \n|\n \n\\\n   \n|\n\n\nnew\n:\n  \n1\n  \n1\n  \n2\n  \n2\n  \n3\n  \n3\n  \n4\n  \n4\n  \n5\n  \n5\n  \n6\n  \n6\n  \n7\n  \n7\n  \n8\n\n\n\n\n\n\nThe impulse responses in the \nnew\n signal are combined by overlapping and adding their components. In Python, overlap and add is simply:\n\n\noriginal_signal\n \n=\n \nnp\n.\nzeros\n(\nN\n)\n\n\n\n# Window, compute impulse response\n\n\n...\n \n\n\nfor\n \nnew_epoch_idx\n \nin\n \nrange\n(\n0\n,\n \nN\n,\n \nP_1\n):\n\n    \noriginal_signal\n[\nnew_epoch_idx\n \n-\n \nP_1\n:\nnew_epoch_idx\n \n+\n \nP_1\n \n+\n \n1\n]\n \n+=\n \nwindowed_response\n\n\n\n\n\n\n\n\nCaution\n\n\nThe code above does not check if array indices are valid, so be careful.\n\n\n\n\nLikewise, consider halving the frequency. This can be achieved with the following mapping:\n\n\norig\n:\n \n1\n     \n2\n     \n3\n     \n4\n     \n5\n     \n6\n     \n7\n     \n8\n\n      \n|\n           \n|\n           \n|\n           \n|\n\n      \n|\n           \n|\n           \n|\n           \n|\n\n\nnew\n:\n  \n1\n           \n3\n           \n5\n           \n7\n\n\n\n\n\n\nOverlap-Add Algorithm\n\n\nThis can be extended further for any multiplier with the following algorithm:\n\n\n\n\nCompute the new period spacing \nP_1 = \\frac{F_s}{F_\\text{new}}\n\n\n\n\nFor every new epoch at \ni = \\left\\{P_1, 2 P_1, 3 P_1, \\dots\\right\\}\n:\n\n\nFind the closest epoch in the original signal\n\n\nApproximate the impulse response by applying a Hanning window of length \n2 P_0 + 1\n centered at the original epoch\n\n\nOverlap and add the windowed epoch into your new buffer centered at index \ni\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\nThis video on doing TD-PSOLA by hand\n is very helpful for getting an intuitive understanding of TD-PSOLA. \n\n\n\n\nBuffer Manipulation\n\n\nTD-PSOLA can be done on an arbitrarily long signal (as you will do in Python), but to run this algorithm in real-time, we need to use buffers. Our autocorrelation-based pitch detector requires at least 40 ms buffers, and we do not want to add any more delay than we have to, so let's keep 40 ms buffers. Our setup then is:\n\n\nbufferIn:  <-- 40 ms -->\n\nbufferOut: <-- 40 ms -->\n\n\n\n\n\nConsider what happens if we do not do anything to ensure buffer continuity. Overlap-added Hanning windows do permit a perfect reconstruction (if we try to generate a synthesis signal with \nP_0 = P_1\n), but note what happens when we have a non-integer number of epochs in our original signal:\n\n\n\n\nEverything beyond sample index 16 is perfectly reconstructed, but the samples prior are missing information for perfect reconstruction. If we do this across multiple buffers, we get a reconstruction waveform with clipping at the boundaries:\n\n\n\n\nThis problem does not come up when pitch shifting an entire .wav file, for example, because offline processing is allowed to be acausal. In other words, when doing this in Python, it is permissible to look into the future for your next epoch.\n\n\nYou cannot look into the future with online processing, but you can delay your buffer slightly to fake having access to past, present, and future data.\n\n\nbufferIn:  | <-- 20 ms past --> | <-- 20 ms present --> | <--     20 ms future     --> |\n\nbufferOut: | <-- 20 ms past --> | <-- 20 ms present --> | <-- 20 ms future (zeros) --> |\n\n\n\n\n\nEvery time we get a new 20 ms frame of data, we shift it into the \nbufferIn\n queue as in Lab 2. Delaying our \npresent\n buffer by 20 ms, we can then look \"into the future\" for our epoch computation.\n\n\nThe trick is then how we deal with \nbufferOut\n. For the TD-PSOLA algorithm itself, we only compute new epochs for the \npresent\n buffer. However, we let the \npresent\n epochs spill over into the \npast\n and \nfuture\n buffers when doing overlap-add.\n\n\nFor example, say we have the following \nbufferIn\n, and we have epochs at \n0, 2, 4, 6, 8, 10, 12, 14\n with \nP_0 = 1\n. \n\n\nbufferIn\n:\n  \n0\n   \n1\n   \n2\n   \n3\n   \n4\n \n|\n \n5\n   \n6\n   \n7\n   \n8\n   \n9\n \n|\n \n10\n  \n11\n  \n12\n  \n13\n  \n14\n\n\n\n\n\n\nIf we want to perfectly reconstruct the signal, we can use the same epochs as in the original signal. Because we only compute \npresent\n buffer epoch, we only compute overlap-add for epochs \n6, 8\n. Assume that this iteration had run before, and the \npast\n buffer is already computed. \n(N)\n denotes an incomplete reconstruction.\n\n\nbufferIn\n:\n  \n0\n   \n1\n   \n2\n   \n3\n   \n4\n \n|\n \n5\n   \n6\n   \n7\n   \n8\n   \n9\n \n|\n \n10\n  \n11\n  \n12\n  \n13\n  \n14\n\n\n\nbufferOut\n:\n \n0\n   \n1\n   \n2\n   \n3\n   \n4\n \n|(\n5\n)\n  \n0\n   \n0\n   \n0\n   \n0\n \n|\n \n0\n   \n0\n   \n0\n   \n0\n   \n0\n\n                          \n+\nwin\n(\n5\n   \n6\n   \n7\n)\n\n                                  \n+\nwin\n(\n7\n   \n8\n   \n9\n)\n\n         \n=\n \n0\n   \n1\n   \n2\n   \n3\n   \n4\n \n|\n \n5\n   \n6\n   \n7\n   \n8\n  \n(\n9\n)|\n \n0\n   \n0\n   \n0\n   \n0\n   \n0\n\n\n\n\n\n\nNotice how \n6, 7, 8\n are perfectly reconstructed, but \n9\n did not yet get enough information for complete reconstruction. Our \npast\n buffer is perfectly reconstructed, however, so we output that buffer to the speaker and shift our \nbufferOut\n accordingly. In the next iteration, we have the following buffer configuration:\n\n\nbufferIn\n:\n  \n5\n   \n6\n   \n7\n   \n8\n   \n9\n \n|\n \n10\n  \n11\n  \n12\n  \n13\n  \n14\n|\n \n15\n  \n16\n  \n17\n  \n18\n  \n19\n\n\n\nbufferOut\n:\n \n5\n   \n6\n   \n7\n   \n8\n  \n(\n9\n)|\n \n0\n   \n0\n   \n0\n   \n0\n   \n0\n \n|\n \n0\n   \n0\n   \n0\n   \n0\n   \n0\n\n\n\n\n\n\nTrying to reconstruct the \npresent\n buffer again, we have the following:\n\n\nbufferIn\n:\n  \n5\n   \n6\n   \n7\n   \n8\n   \n9\n \n|\n \n10\n  \n11\n  \n12\n  \n13\n  \n14\n|\n \n15\n  \n16\n  \n17\n  \n18\n  \n19\n\n\n\nbufferOut\n:\n \n5\n   \n6\n   \n7\n   \n8\n  \n(\n9\n)|\n \n0\n   \n0\n   \n0\n   \n0\n   \n0\n \n|\n \n0\n   \n0\n   \n0\n   \n0\n   \n0\n\n                      \n+\nwin\n(\n9\n   \n10\n  \n11\n)\n\n                              \n+\nwin\n(\n11\n  \n12\n  \n13\n)\n\n                                      \n+\nwin\n(\n13\n  \n14\n  \n15\n)\n\n         \n=\n \n5\n   \n6\n   \n7\n   \n8\n   \n9\n \n|\n \n10\n  \n11\n  \n12\n  \n13\n  \n14\n|(\n15\n)\n \n0\n   \n0\n   \n0\n   \n0\n\n\n\n\n\n\nBy allowing our \npresent\n computation to spill over into the \npast\n and \nfuture\n buffers, we can guarantee that by the end of your \npresent\n computation, the \npast\n buffer will be fully reconstructed. \n\n\nBuffer Manipulation Algorithm\n\n\nTo implement this practically, the algorithm is as follows. Let \nFRAME_SIZE\n be the number of samples for each 20 ms section, and let \ni\n point to your new epoch positions.\n\n\n\n\nInitialize \ni = FRAME_SIZE\n so it points at the first index of the \npresent\n section of \nbufferOut\n. Only do this in the first iteration.\n\n\nWhile \ni < 2 * FRAME_SIZE\n, or while \ni\n is inside \npresent\n:\n\n\nCompute the \noverlap-add algorithm detailed above\n\n\nIncrement \ni\n such that \ni += P_1\n\n\n\n\n\n\nDecrement \ni\n such that \ni -= FRAME_SIZE\n\n\n\n\nLetting \ni\n spill over into the \nfuture\n buffer (which is what happens in the last iteration of the while loop), we maintain a pointer to where the first epoch of the next frame should go. Decrementing by \nFRAME_SIZE\n ensures continuity after shifting your \npast\n buffer out to the speaker.\n\n\n\n\nNote\n\n\nYour TA will go over this in more detail at the beginning of your lab section. \nDon't panic!",
            "title": "Lab 5 - Pitch Synthesis"
        },
        {
            "location": "/lab5/lab/#lab-5-pitch-synthesis",
            "text": "",
            "title": "Lab 5 - Pitch Synthesis"
        },
        {
            "location": "/lab5/lab/#summary",
            "text": "In this lab, you will learn about a simplified model for human speech and how to exploit this model to shift the frequency of an incoming signal. You will also learn one method for producing continuous output within a batch processing framework.",
            "title": "Summary"
        },
        {
            "location": "/lab5/lab/#downloads",
            "text": "Python test harness and test vector TBA  Android project source code TBA    Some Remarks  We have completed the scaffolding of the TD-PSOLA algorithm for you, leaving remapping, windowing, and overlap-add for your lab assignment. You will be responsible for knowing why our buffering scheme guarantees smoothness in the signal reconstruction, and you will also be responsible for knowing why the characteristics of human speech permit such an algorithm.  We will try to explain these in detail in the lab notes, but please do not hesitate to ask your TA or ask on Piazza if these details are unclear.",
            "title": "Downloads"
        },
        {
            "location": "/lab5/lab/#td-psola",
            "text": "Time-Domain Pitch Synchronous Overlap-Add, or TD-PSOLA, is the pitch-shifting algorithm we will be using in this lab. It relies heavily the source-filter model of speech, detailed below.",
            "title": "TD-PSOLA"
        },
        {
            "location": "/lab5/lab/#source-filter-model-of-speech",
            "text": "We can model speech as the output of two successive blocks, a source block and filter block:   Our vocal cords are the source, and we model the output of our vocal cords as a delta train (also called a Dirac comb). Of course, our vocal cords cannot truly produce zero-width impulses of energy, but this assumption is sufficient for modeling purposes. This series of impulses determines the fundamental frequency, or the pitch, of our utterance.   The impulses of energy produced by our vocal cords then travel up through our vocal tract. Everything in your vocal tract (lips, tongue throat, mouth cavity, nasal cavity, among others) affect the final waveform. We make the assumption that these effects are linear and can be combined without penalty, so we call everything inside the vocal tract the  filter .  Simplifying the complex problem of speech generation in this manner allows us to apply some signal processing tools we learned in ECE 310. In particular, if we assume that speech is nothing but the output of a filter, we can rewrite speech in the form of a transfer function where  y(t)  is the utterance,  x(t)  is the source, and  h(t)  is the filter:   \ny(t) = x(t) * h(t)   Note that  *  denotes the convolution operator. We can further rewrite  x(t) , assuming  x(t)  is a delta train with some period  P :    \n\\begin{align}\n    y(t) &= \\left(\\sum_{k=0}^\\infty \\delta(t - k P)\\right) * h(t) \\\\\n         &= \\sum_{k=0}^\\infty \\delta(t - k P) * h(t) \\\\\n         &= \\sum_{k=0}^\\infty h(t - k P)\n\\end{align}       This is a powerful result which says that the fundamental frequency (the pitch) does not depend on the filter  h(t) , but on the source  x(t) . In other words, speech can be decomposed into a sum of impulse responses, where the spacing of the impulse responses  P  determines the pitch.   If we want to shift the pitch, all we need is some way of identifying the impulse response from the speech signal. We can then extract the impulse response and place it with any spacing  P  in the synthesis signal we desire.",
            "title": "Source-Filter Model of Speech"
        },
        {
            "location": "/lab5/lab/#td-psola-explained",
            "text": "Considering the discussion above, we can rewrite our problem more formally. Given a signal  y_{P_0}(t)  with some fundamental period  P_0 :   \ny_{P_0}(t) = \\sum_{k=0}^\\infty h(t - k P_0),   compute a new synthesis signal  y_{P_1}(t)  with a new period  P_1 :   \n\\begin{align}\n    y_{P_1}(t) &= \\sum_{k=0}^\\infty h(t - k P_1).\n\\end{align}   In order to compute the new synthesis signal, we need some way of extracting the impulse response from  y_{P_0}(t) . This is done by identifying the period (using the technique from Lab 4) and further identifying each  epoch  in the signal, or the location of each pitch period, by finding peaks in the signal approximately  P_0  samples apart. An example of this for a sample frame is shown below.   We can then extract the impulse response (rather, an estimate of the impulse response) by windowing  \\pm P_0  about each epoch marker. For example, when windowed with a Hanning window, an individual response may look like this:   Now that we have a method of estimating the impulse responses, we simply need to map from the original spacing  y_{P_0}(t) = \\sum_{k=0}^\\infty h(t - k P_0)  to the synthesis spacing  y_{P_1}(t) = \\sum_{k=0}^\\infty h(t - k P_1) . In other words, we need to determine where the original impulse responses should go in our new signal.  Imagine our original signal has epochs spaced as below:  orig :   1       2       3       4       5       6       7       8   and we want to double the frequency of the original signal without changing the duration. This can be achieved by spacing our new epochs at  P_1 = \\frac{P_0}{2} . For every new epoch at  \\left\\{ P_1, 2 P_1, 3 P_1, \\dots \\right\\} , we need to find the nearest epoch in the original signal. This mapping is shown below for our example signal:  orig :   1       2       3       4       5       6       7       8 \n       |\\      |\\      |\\      |\\      |\\      |\\      |\\      | \n       |   \\     |   \\     |   \\     |   \\     |   \\     |   \\     |   \\     |  new :    1    1    2    2    3    3    4    4    5    5    6    6    7    7    8   The impulse responses in the  new  signal are combined by overlapping and adding their components. In Python, overlap and add is simply:  original_signal   =   np . zeros ( N )  # Window, compute impulse response  ...   for   new_epoch_idx   in   range ( 0 ,   N ,   P_1 ): \n     original_signal [ new_epoch_idx   -   P_1 : new_epoch_idx   +   P_1   +   1 ]   +=   windowed_response    Caution  The code above does not check if array indices are valid, so be careful.   Likewise, consider halving the frequency. This can be achieved with the following mapping:  orig :   1       2       3       4       5       6       7       8 \n       |             |             |             | \n       |             |             |             |  new :    1             3             5             7",
            "title": "TD-PSOLA Explained"
        },
        {
            "location": "/lab5/lab/#overlap-add-algorithm",
            "text": "This can be extended further for any multiplier with the following algorithm:   Compute the new period spacing  P_1 = \\frac{F_s}{F_\\text{new}}   For every new epoch at  i = \\left\\{P_1, 2 P_1, 3 P_1, \\dots\\right\\} :  Find the closest epoch in the original signal  Approximate the impulse response by applying a Hanning window of length  2 P_0 + 1  centered at the original epoch  Overlap and add the windowed epoch into your new buffer centered at index  i       Tip  This video on doing TD-PSOLA by hand  is very helpful for getting an intuitive understanding of TD-PSOLA.",
            "title": "Overlap-Add Algorithm"
        },
        {
            "location": "/lab5/lab/#buffer-manipulation",
            "text": "TD-PSOLA can be done on an arbitrarily long signal (as you will do in Python), but to run this algorithm in real-time, we need to use buffers. Our autocorrelation-based pitch detector requires at least 40 ms buffers, and we do not want to add any more delay than we have to, so let's keep 40 ms buffers. Our setup then is:  bufferIn:  <-- 40 ms -->\n\nbufferOut: <-- 40 ms -->  Consider what happens if we do not do anything to ensure buffer continuity. Overlap-added Hanning windows do permit a perfect reconstruction (if we try to generate a synthesis signal with  P_0 = P_1 ), but note what happens when we have a non-integer number of epochs in our original signal:   Everything beyond sample index 16 is perfectly reconstructed, but the samples prior are missing information for perfect reconstruction. If we do this across multiple buffers, we get a reconstruction waveform with clipping at the boundaries:   This problem does not come up when pitch shifting an entire .wav file, for example, because offline processing is allowed to be acausal. In other words, when doing this in Python, it is permissible to look into the future for your next epoch.  You cannot look into the future with online processing, but you can delay your buffer slightly to fake having access to past, present, and future data.  bufferIn:  | <-- 20 ms past --> | <-- 20 ms present --> | <--     20 ms future     --> |\n\nbufferOut: | <-- 20 ms past --> | <-- 20 ms present --> | <-- 20 ms future (zeros) --> |  Every time we get a new 20 ms frame of data, we shift it into the  bufferIn  queue as in Lab 2. Delaying our  present  buffer by 20 ms, we can then look \"into the future\" for our epoch computation.  The trick is then how we deal with  bufferOut . For the TD-PSOLA algorithm itself, we only compute new epochs for the  present  buffer. However, we let the  present  epochs spill over into the  past  and  future  buffers when doing overlap-add.  For example, say we have the following  bufferIn , and we have epochs at  0, 2, 4, 6, 8, 10, 12, 14  with  P_0 = 1 .   bufferIn :    0     1     2     3     4   |   5     6     7     8     9   |   10    11    12    13    14   If we want to perfectly reconstruct the signal, we can use the same epochs as in the original signal. Because we only compute  present  buffer epoch, we only compute overlap-add for epochs  6, 8 . Assume that this iteration had run before, and the  past  buffer is already computed.  (N)  denotes an incomplete reconstruction.  bufferIn :    0     1     2     3     4   |   5     6     7     8     9   |   10    11    12    13    14  bufferOut :   0     1     2     3     4   |( 5 )    0     0     0     0   |   0     0     0     0     0 \n                           + win ( 5     6     7 ) \n                                   + win ( 7     8     9 ) \n          =   0     1     2     3     4   |   5     6     7     8    ( 9 )|   0     0     0     0     0   Notice how  6, 7, 8  are perfectly reconstructed, but  9  did not yet get enough information for complete reconstruction. Our  past  buffer is perfectly reconstructed, however, so we output that buffer to the speaker and shift our  bufferOut  accordingly. In the next iteration, we have the following buffer configuration:  bufferIn :    5     6     7     8     9   |   10    11    12    13    14 |   15    16    17    18    19  bufferOut :   5     6     7     8    ( 9 )|   0     0     0     0     0   |   0     0     0     0     0   Trying to reconstruct the  present  buffer again, we have the following:  bufferIn :    5     6     7     8     9   |   10    11    12    13    14 |   15    16    17    18    19  bufferOut :   5     6     7     8    ( 9 )|   0     0     0     0     0   |   0     0     0     0     0 \n                       + win ( 9     10    11 ) \n                               + win ( 11    12    13 ) \n                                       + win ( 13    14    15 ) \n          =   5     6     7     8     9   |   10    11    12    13    14 |( 15 )   0     0     0     0   By allowing our  present  computation to spill over into the  past  and  future  buffers, we can guarantee that by the end of your  present  computation, the  past  buffer will be fully reconstructed.",
            "title": "Buffer Manipulation"
        },
        {
            "location": "/lab5/lab/#buffer-manipulation-algorithm",
            "text": "To implement this practically, the algorithm is as follows. Let  FRAME_SIZE  be the number of samples for each 20 ms section, and let  i  point to your new epoch positions.   Initialize  i = FRAME_SIZE  so it points at the first index of the  present  section of  bufferOut . Only do this in the first iteration.  While  i < 2 * FRAME_SIZE , or while  i  is inside  present :  Compute the  overlap-add algorithm detailed above  Increment  i  such that  i += P_1    Decrement  i  such that  i -= FRAME_SIZE   Letting  i  spill over into the  future  buffer (which is what happens in the last iteration of the while loop), we maintain a pointer to where the first epoch of the next frame should go. Decrementing by  FRAME_SIZE  ensures continuity after shifting your  past  buffer out to the speaker.   Note  Your TA will go over this in more detail at the beginning of your lab section.  Don't panic!",
            "title": "Buffer Manipulation Algorithm"
        },
        {
            "location": "/resources/python/",
            "text": "Python Resources\n\n\nGeneral\n\n\nThis website\n has a great set of cheat sheets for general Python knowledge.\n\n\nNumpy also has a great \nNumpy for Matlab users\n section which compares common MATLAB commands to their equivalent expressions in Numpy.\n\n\nMatplotlib has a tutorial on their \npyplot\n plotting system \nhere\n. Their syntax is generally the same as MATLAB's plotting syntax. but you prepend \nplt.\n to most commands. For example, a basic matplotlib plot looks like:\n\n\nimport\n \nmatplotlib.pyplot\n \nas\n \nplt\n\n\n\nplt\n.\nplot\n([\n1\n,\n \n2\n,\n \n3\n,\n \n4\n])\n\n\nplt\n.\nylabel\n(\n'some numbers'\n)\n\n\nplt\n.\nshow\n()\n\n\n\n\n\n\nLast but not least, the official \nPython docs\n should be your go-to for Python questions if the cheatsheets above aren't sufficient.\n\n\nDSP\n\n\nScipy's signal processing library documentation can be found \nhere\n.\n\n\nA brief cheatsheet\n\n\nConditionals\n\n\na\n \n=\n \n5\n\n\nb\n \n=\n \n5\n \n\nc\n \n=\n \n8\n\n\n\nif\n \na\n \n==\n \nb\n \nor\n \nb\n \n!=\n \nc\n:\n \n    \n# ... \n\n\n\n\n\n\nLoops\n\n\nfor\n \ni\n \nin\n \nrange\n(\n10\n):\n\n    \n# i = 0, 1, 2, 3, 4, 5, 6, 7, 8, 9\n\n\n\nfor\n \ni\n \nin\n \nrange\n(\n5\n,\n \n10\n):\n\n    \n# i = 5, 6, 7, 8, 9\n\n\n\nj\n \n=\n \n10\n\n\nwhile\n \nj\n \n>=\n \n0\n:\n\n    \n# ...\n\n    \nj\n \n-=\n \n1\n\n\n\nwhile\n \nTrue\n:\n \n    \n# ...\n\n\n\n\n\n\nPython Arrays\n\n\na\n \n=\n \n[\n1\n,\n \n2\n,\n \n3\n,\n \n4\n,\n \n5\n]\n\n\n\n# a[0] == 1 \n\n\n# a[1] == 2 \n\n\n# ...\n\n\n# a[-1] == a[N - 1] == 5\n\n\n# a[-2] == a[N - 2] == 4\n\n\n\nb\n \n=\n \n[]\n\n\n\nb\n.\nappend\n(\n6\n)\n\n\nb\n.\nappend\n(\n7\n)\n\n\nb\n.\nappend\n(\n8\n)\n\n\n\n# len(b) == 3\n\n\n# b[0] == 6\n\n\n\n\n\n\nFunction definitions\n\n\ndef\n \nyourNewFunction\n(\nrequiredArg1\n,\n \nrequiredArg2\n,\n \noptionalArg3\n=\ndefaultValue\n):\n\n    \n# ... \n\n    \nreturn\n \n0\n\n\n\n\n\n\nNumpy arrays\n\n\nimport\n \nnumpy\n \nas\n \nnp\n \n\n\n# Numpy arrays are quicker than python arrays (actually called lists) \n\n\n# because they are preallocated. This means that a.append() does \n\n\n# not work for numpy arrays.\n\n\na\n \n=\n \nnp\n.\narray\n([\n1\n,\n \n2\n,\n \n3\n,\n \n4\n,\n \n5\n])\n\n\n\n# array access is the same \n\n\n# a[0] == 1 \n\n\n# ... \n\n\n\nb\n \n=\n \nnp\n.\nzeros\n(\n5\n)\n\n\n\n# b.shape == (5, )\n\n\n# This is a numpy vector of length 5 \n\n\n\nc\n \n=\n \nnp\n.\nones\n(\n5\n,\n \n2\n)\n\n\n\n# c.shape == (5, 2)\n\n\n# This is a numpy array with 5 rows and two columns\n\n\n\n\n\n\nPlotting\n\n\nimport\n \nmatplotlib.pyplot\n \nas\n \nplt\n \n\n\nt\n \n=\n \nnp\n.\narray\n([\ni\n \nfor\n \ni\n \nin\n \nrange\n(\n100\n)])\n\n\nx\n \n=\n \nnp\n.\ncos\n(\nt\n)\n\n\n\nplt\n.\nfigure\n()\n\n\nplt\n.\nplot\n(\nt\n,\n \nx\n)\n\n\nplt\n.\nxlabel\n(\n'index'\n)\n\n\nplt\n.\nylabel\n(\n'x'\n)\n\n\nplt\n.\ntitle\n(\n'A random plot'\n)\n\n\nplt\n.\nshow\n()\n  \n\n\n\n\n\nFile reading/writing\n\n\nimport\n \nnumpy\n \nas\n \nnp\n \n\n\n# For CSV read specifically\n\n\nfilename\n \n=\n \n'csv_sample.csv'\n\n\ndata\n \n=\n \nnp\n.\ngenfromtxt\n(\nfilename\n,\n \ndelimiter\n=\n','\n)\n\n\n\n# For general file read\n\n\nwith\n \nopen\n(\nfilename\n,\n \n'r'\n)\n \nas\n \nf\n:\n\n    \ndata\n \n=\n \nf\n.\nreadlines\n()\n\n\n\n# For general file write\n\n\nwith\n \nopen\n(\nfilename\n,\n \n'w'\n)\n \nas\n \nf\n:\n\n    \nf\n.\nwrite\n(\n'Hello world!'\n)\n\n\n\n# Using \"with\" automatically handles opening/closing file descriptors",
            "title": "Python"
        },
        {
            "location": "/resources/python/#python-resources",
            "text": "",
            "title": "Python Resources"
        },
        {
            "location": "/resources/python/#general",
            "text": "This website  has a great set of cheat sheets for general Python knowledge.  Numpy also has a great  Numpy for Matlab users  section which compares common MATLAB commands to their equivalent expressions in Numpy.  Matplotlib has a tutorial on their  pyplot  plotting system  here . Their syntax is generally the same as MATLAB's plotting syntax. but you prepend  plt.  to most commands. For example, a basic matplotlib plot looks like:  import   matplotlib.pyplot   as   plt  plt . plot ([ 1 ,   2 ,   3 ,   4 ])  plt . ylabel ( 'some numbers' )  plt . show ()   Last but not least, the official  Python docs  should be your go-to for Python questions if the cheatsheets above aren't sufficient.",
            "title": "General"
        },
        {
            "location": "/resources/python/#dsp",
            "text": "Scipy's signal processing library documentation can be found  here .",
            "title": "DSP"
        },
        {
            "location": "/resources/python/#a-brief-cheatsheet",
            "text": "",
            "title": "A brief cheatsheet"
        },
        {
            "location": "/resources/python/#conditionals",
            "text": "a   =   5  b   =   5   c   =   8  if   a   ==   b   or   b   !=   c :  \n     # ...",
            "title": "Conditionals"
        },
        {
            "location": "/resources/python/#loops",
            "text": "for   i   in   range ( 10 ): \n     # i = 0, 1, 2, 3, 4, 5, 6, 7, 8, 9  for   i   in   range ( 5 ,   10 ): \n     # i = 5, 6, 7, 8, 9  j   =   10  while   j   >=   0 : \n     # ... \n     j   -=   1  while   True :  \n     # ...",
            "title": "Loops"
        },
        {
            "location": "/resources/python/#python-arrays",
            "text": "a   =   [ 1 ,   2 ,   3 ,   4 ,   5 ]  # a[0] == 1   # a[1] == 2   # ...  # a[-1] == a[N - 1] == 5  # a[-2] == a[N - 2] == 4  b   =   []  b . append ( 6 )  b . append ( 7 )  b . append ( 8 )  # len(b) == 3  # b[0] == 6",
            "title": "Python Arrays"
        },
        {
            "location": "/resources/python/#function-definitions",
            "text": "def   yourNewFunction ( requiredArg1 ,   requiredArg2 ,   optionalArg3 = defaultValue ): \n     # ...  \n     return   0",
            "title": "Function definitions"
        },
        {
            "location": "/resources/python/#numpy-arrays",
            "text": "import   numpy   as   np   # Numpy arrays are quicker than python arrays (actually called lists)   # because they are preallocated. This means that a.append() does   # not work for numpy arrays.  a   =   np . array ([ 1 ,   2 ,   3 ,   4 ,   5 ])  # array access is the same   # a[0] == 1   # ...   b   =   np . zeros ( 5 )  # b.shape == (5, )  # This is a numpy vector of length 5   c   =   np . ones ( 5 ,   2 )  # c.shape == (5, 2)  # This is a numpy array with 5 rows and two columns",
            "title": "Numpy arrays"
        },
        {
            "location": "/resources/python/#plotting",
            "text": "import   matplotlib.pyplot   as   plt   t   =   np . array ([ i   for   i   in   range ( 100 )])  x   =   np . cos ( t )  plt . figure ()  plt . plot ( t ,   x )  plt . xlabel ( 'index' )  plt . ylabel ( 'x' )  plt . title ( 'A random plot' )  plt . show ()",
            "title": "Plotting"
        },
        {
            "location": "/resources/python/#file-readingwriting",
            "text": "import   numpy   as   np   # For CSV read specifically  filename   =   'csv_sample.csv'  data   =   np . genfromtxt ( filename ,   delimiter = ',' )  # For general file read  with   open ( filename ,   'r' )   as   f : \n     data   =   f . readlines ()  # For general file write  with   open ( filename ,   'w' )   as   f : \n     f . write ( 'Hello world!' )  # Using \"with\" automatically handles opening/closing file descriptors",
            "title": "File reading/writing"
        },
        {
            "location": "/resources/android/",
            "text": "Android Resources\n\n\nThe official \nAndroid documentation\n should always be your first stop for Android information. Their \"Building Your First App\" tutorial should be enough to learn the Android lifecycle and general project structure.\n\n\nGoogle also has an official \nAndroid Sensors Overview\n which discusses how to use all of the onboard sensors, including the accelerometer and gyroscope. Most of this code will be written for you already in Lab 1, but you may find this useful during your final project.\n\n\nJava is very similar to C/C++, but you may still find \nthis Java cheatsheet\n useful. Apologies for the horrid color scheme.\n\n\nIf you are interested in how the Android C++ NDK works, Google has a writeup \nhere\n.",
            "title": "Android"
        },
        {
            "location": "/resources/android/#android-resources",
            "text": "The official  Android documentation  should always be your first stop for Android information. Their \"Building Your First App\" tutorial should be enough to learn the Android lifecycle and general project structure.  Google also has an official  Android Sensors Overview  which discusses how to use all of the onboard sensors, including the accelerometer and gyroscope. Most of this code will be written for you already in Lab 1, but you may find this useful during your final project.  Java is very similar to C/C++, but you may still find  this Java cheatsheet  useful. Apologies for the horrid color scheme.  If you are interested in how the Android C++ NDK works, Google has a writeup  here .",
            "title": "Android Resources"
        }
    ]
}